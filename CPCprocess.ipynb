{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPCprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z57IXf9q1Ovh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "1b2cbade-88b2-48d7-b64c-3ab7d29c1715"
      },
      "source": [
        "!pip install pybullet\n",
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.8.4)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t8ezUpd1SIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np \n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "import pybullet_envs\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0_9dbk8Cbnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4cdb116a-5cec-4456-dc79-f07bb48d9fe3"
      },
      "source": [
        "np_seed = 654765645\n",
        "tf_seed = 776644345\n",
        "np.random.seed(np_seed)\n",
        "tf.random.set_seed(tf_seed)\n",
        "\n",
        "# check if GPU\n",
        "tf.config.list_physical_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1Ruv5FCgDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fe5a6b2-b4e6-4241-e5ca-c412cf4637a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_dir = \"drive/My Drive/\"\n",
        "base_dir = root_dir + 'CPCtesting'\n",
        "os.makedirs(base_dir,exist_ok=True)\n",
        "\n",
        "train_dir = base_dir + '/train'\n",
        "os.makedirs(train_dir,exist_ok=True)\n",
        "\n",
        "model_dir = base_dir + '/model'\n",
        "os.makedirs(model_dir,exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWEWANi66t_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW6bj9SJkd20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A2C(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(A2C,self).__init__()\n",
        "    self.actor\n",
        "    self.critic\n",
        "  def optimizer(self):\n",
        "    pass\n",
        "  def loss(self):\n",
        "    pass\n",
        "  def call(self):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StY2xL3E73ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataHandler:\n",
        "    def __init__(self, batch_size, terms, predict_terms=1, image_size=64, color=False, rescale=True, aug=True, is_training=True, method='cpc'):\n",
        "        self.batch_size = batch_size\n",
        "        self.terms = terms\n",
        "        self.predict_terms = predict_terms\n",
        "        self.image_size = image_size\n",
        "        self.color = color\n",
        "        self.rescale = rescale\n",
        "        self.aug = aug\n",
        "        self.is_training = is_training\n",
        "        self.method = method\n",
        "        self.lena = cv2.imread(os.path.join(base_dir,'lena.jpg'))\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "        if self.is_training:\n",
        "            self.x = x_train/255.0\n",
        "            self.y = y_train\n",
        "        else:\n",
        "            self.x = x_test/255.0\n",
        "            self.y = y_test\n",
        "        self.idxs = []\n",
        "        for i in range(10):\n",
        "            y = y_train if self.is_training else y_test\n",
        "            self.idxs.append(np.where(y == i)[0])\n",
        "        self.n_samples = len(self.y)//terms if self.method == 'cpc' else len(self.y)\n",
        "        self.shape = self.x.shape\n",
        "        self.n_batches = self.n_samples//batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.cpc_batch() if self.method == 'cpc' else self.benchmark_batch()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "    def cpc_batch(self):\n",
        "        img_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
        "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
        "        for bi in range(self.batch_size):\n",
        "            seed = np.random.randint(10)\n",
        "            sentence = np.arange(seed, seed + self.terms + self.predict_terms) % 10\n",
        "            if bi < self.batch_size//2:\n",
        "                num = np.arange(10)\n",
        "                predicted = sentence[-self.predict_terms:]\n",
        "                for i, p in enumerate(predicted):\n",
        "                    predicted[i] = np.random.choice(num[num != p], 1)\n",
        "                sentence[-self.predict_terms:] = predicted % 10\n",
        "                sentence_labels[bi, :] = 0\n",
        "            img_labels[bi, :] = sentence\n",
        "        images = self.get_samples(img_labels).reshape((self.batch_size, self.terms+self.predict_terms, self.image_size, self.image_size, 3))\n",
        "        x_images = images[:, :-self.predict_terms, ...]\n",
        "        y_images = images[:, -self.predict_terms:, ...]\n",
        "        idx = np.random.choice(self.batch_size, self.batch_size, replace=False)\n",
        "        return [x_images[idx], y_images[idx]], sentence_labels[idx]\n",
        "\n",
        "    def get_samples(self, img_labels):\n",
        "        idx = []\n",
        "        for label in img_labels.flatten():\n",
        "            idx.append(np.random.choice(self.idxs[int(label)], 1)[0])\n",
        "        img_batch = self.x[idx, :, :]\n",
        "        if self.aug:\n",
        "            img_batch = self._aug_batch(img_batch)\n",
        "        return img_batch\n",
        "\n",
        "    def _aug_batch(self, img_batch):\n",
        "        if self.image_size != 28:\n",
        "            resized = []\n",
        "            for i in range(img_batch.shape[0]):\n",
        "                resized.append(cv2.resize(img_batch[i], (self.image_size, self.image_size)))\n",
        "            img_batch = np.stack(resized)\n",
        "        img_batch = img_batch.reshape((img_batch.shape[0], 1, self.image_size, self.image_size))\n",
        "        img_batch = np.concatenate([img_batch, img_batch, img_batch], axis=1)\n",
        "\n",
        "        if self.color:\n",
        "            img_batch[img_batch >= 0.5] = 1\n",
        "            img_batch[img_batch < 0.5] = 0\n",
        "            for i in range(img_batch.shape[0]):\n",
        "                x_c = np.random.randint(0, self.lena.shape[0] - self.image_size)\n",
        "                y_c = np.random.randint(0, self.lena.shape[1] - self.image_size)\n",
        "                img = self.lena[x_c:x_c+self.image_size, y_c:y_c+self.image_size]\n",
        "                img = np.array(img).transpose((2, 0, 1))/255.0\n",
        "                for j in range(3):\n",
        "                    img[j, :, :] = (img[j, :, :] + np.random.uniform(0, 1))/2.0\n",
        "                img[img_batch[i, :, :, :] == 1] = 1 - img[img_batch[i, :, :, :] == 1]\n",
        "                img_batch[i, :, :, :] = img\n",
        "\n",
        "        if self.rescale:\n",
        "            img_batch = img_batch * 2 - 1\n",
        "        img_batch = img_batch.transpose((0, 2, 3, 1))\n",
        "        return img_batch\n",
        "\n",
        "    def benchmark_batch(self):\n",
        "        idx = np.random.choice(len(self.x), self.batch_size, replace=False)\n",
        "        img_batch = self.x[idx]\n",
        "        label_batch = self.y[idx]\n",
        "        if self.aug:\n",
        "            img_batch = self._aug_batch(img_batch)\n",
        "        label_batch = label_batch.reshape((-1, 1))\n",
        "        return img_batch, label_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIKY-VyC1bsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CPCModel(tf.keras.Model):\n",
        "  def __init__(self,code_size, predict_terms, terms=4, units=256, image_size=64, channels=3):\n",
        "      super(CPCModel, self).__init__()\n",
        "      self.code_size = code_size\n",
        "      self.predict_terms = predict_terms\n",
        "      self.terms = terms\n",
        "      self.units = units\n",
        "      self.image_size = image_size\n",
        "      self.channels = channels\n",
        "\n",
        "      self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
        "      self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "      self.lrelu1 = tf.keras.layers.LeakyReLU()\n",
        "      self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
        "      self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "      self.lrelu2 = tf.keras.layers.LeakyReLU()\n",
        "      self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
        "      self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "      self.lrelu3 = tf.keras.layers.LeakyReLU()\n",
        "      self.conv4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
        "      self.bn4 = tf.keras.layers.BatchNormalization()\n",
        "      self.lrelu4 = tf.keras.layers.LeakyReLU()\n",
        "      self.flatten = tf.keras.layers.Flatten()\n",
        "      self.dense5 = tf.keras.layers.Dense(units=256, activation='linear')\n",
        "      self.bn5 = tf.keras.layers.BatchNormalization()\n",
        "      self.lrelu5 = tf.keras.layers.LeakyReLU()\n",
        "      self.dense6 = tf.keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')\n",
        "\n",
        "      self.gru = tf.keras.layers.GRU(units, return_sequences=False, name='ar_context')\n",
        "      self.linear = tf.keras.layers.Dense(predict_terms*code_size, activation='linear')    \n",
        "  \n",
        "  def encoding(self,x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.lrelu1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.bn2(x)\n",
        "      x = self.lrelu2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.bn3(x)\n",
        "      x = self.lrelu3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.bn4(x)\n",
        "      x = self.lrelu4(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.dense5(x)\n",
        "      x = self.bn5(x)\n",
        "      x = self.lrelu5(x)\n",
        "      z = self.dense6(x)\n",
        "      return z\n",
        "  \n",
        "  def get_context(self, x):\n",
        "      z = self.encoding(x)\n",
        "      z = tf.reshape(z, [-1, self.terms, self.code_size])\n",
        "      c = self.gru(z)\n",
        "      return c\n",
        "\n",
        "  def get_prediction(self, x):\n",
        "      c = self.get_context(x)\n",
        "      z_hats = self.linear(c)\n",
        "      z_hat = tf.reshape(z_hats, [-1, self.predict_terms, self.code_size])\n",
        "      return z_hat\n",
        "  \n",
        "  def optimizer(self):\n",
        "    pass\n",
        "\n",
        "  def loss(self,weights,biases,labels,inputs,num_samples,num_classes):\n",
        "    \n",
        "    loss = tf.nn.nce_loss(\n",
        "    weights, biases, labels, inputs, num_sampled, num_classes, num_true=1,\n",
        "    sampled_values=None, remove_accidental_hits=False, name='nce_loss')\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def call(self,inputs):\n",
        "    x_tm, x_tp = inputs\n",
        "    x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels])\n",
        "    x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels])\n",
        "    z_hat = self.get_prediction(x_tm)\n",
        "    z_tp = self.encoding(x_tp)\n",
        "    z_tp = tf.reshape(z_tp, [-1, self.predict_terms, self.code_size])\n",
        "    dot_prods = tf.reduce_mean(tf.reduce_mean(z_hat*z_tp, axis=-1), axis=-1, keepdims=True)\n",
        "    probs = tf.sigmoid(dot_prods)\n",
        "    return probs\n",
        "\n",
        "\n",
        "  # def save(self):\n",
        "  #       f1 = os.path.join(folder,'target_actor')\n",
        "  #       f2 = os.path.join(folder, 'target_critic')\n",
        "  #       f3 = os.path.join(folder, 'actor')\n",
        "  #       f4 = os.path.join(folder, 'critic')\n",
        "  #       self.target_actor.save(f1)\n",
        "  #       self.target_critic.save(f2)\n",
        "  #       self.actor.save(f3)\n",
        "  #       self.critic.save(f4)\n",
        "\n",
        "\n",
        "  # def load(self):\n",
        "  #   pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbyX3qlk1b4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ee8316c-0a96-4277-b003-e9937032d726"
      },
      "source": [
        "#train loop\n",
        "dh_train = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=True, method='cpc')\n",
        "dh_test = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=False, method='cpc')\n",
        "accuracy_metric_train = tf.keras.metrics.BinaryAccuracy()\n",
        "loss_metric_train = tf.keras.metrics.BinaryCrossentropy()\n",
        "accuracy_metric_test = tf.keras.metrics.BinaryAccuracy()\n",
        "loss_metric_test = tf.keras.metrics.BinaryCrossentropy()\n",
        "cpc = CPCModel(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
        "optim = tf.keras.optimizers.Adam(1e-3)\n",
        "cb = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
        "      tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
        "                                          monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
        "      tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
        "      tf.keras.callbacks.TensorBoard()]\n",
        "cpc.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "cpc.fit(x=dh_train, epochs=1, validation_data=dh_test, steps_per_epoch=60000//64, validation_steps=10000//64, callbacks=cb)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "280/937 [=======>......................] - ETA: 19:42 - loss: 0.6941 - binary_accuracy: 0.4963"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GebEW0Bf1b_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn5Hapce1cE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aFHYhHwRxH6",
        "colab_type": "text"
      },
      "source": [
        " https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss(InfoNCE Loss )\n",
        "<br>\n",
        "Representation Learning with Contrastive Predictive Coding\n",
        "<br>\n",
        "https://github.com/gdao-research/cpc/blob/master/cpc/data_handler.py (CPC)\n",
        "<br>\n",
        "https://github.com/davidtellez/contrastive-predictive-coding/blob/master/train_model.py (CPC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0eguufNR5p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}