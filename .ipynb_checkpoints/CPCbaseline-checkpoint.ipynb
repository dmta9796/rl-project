{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "z57IXf9q1Ovh",
    "outputId": "1b2cbade-88b2-48d7-b64c-3ab7d29c1715"
   },
   "outputs": [],
   "source": [
    "# if colab\n",
    "\n",
    "# !pip install pybullet\n",
    "# !pip install gym\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t8ezUpd1SIy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np \n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "import pybullet_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "y0_9dbk8Cbnv",
    "outputId": "4cdb116a-5cec-4456-dc79-f07bb48d9fe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_seed = 654765645\n",
    "tf_seed = 776644345\n",
    "np.random.seed(np_seed)\n",
    "tf.random.set_seed(tf_seed)\n",
    "\n",
    "# check if GPU\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hG1Ruv5FCgDv",
    "outputId": "4fe5a6b2-b4e6-4241-e5ca-c412cf4637a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = \"drive/My Drive/\"\n",
    "# base_dir = root_dir + 'CPCtesting'\n",
    "# os.makedirs(base_dir,exist_ok=True)\n",
    "\n",
    "# train_dir = base_dir + '/train'\n",
    "# os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "# model_dir = base_dir + '/model'\n",
    "# os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "# if local machine\n",
    "base_dir = \".\"\n",
    "\n",
    "train_dir = base_dir + '/train'\n",
    "os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "model_dir = base_dir + '/model'\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "logs_base_dir = base_dir + '/logs'\n",
    "# tensorboard\n",
    "%load_ext tensorboard\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWEWANi66t_w"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIKY-VyC1bsu"
   },
   "outputs": [],
   "source": [
    "class CPCModel(tf.keras.Model):\n",
    "    def __init__(self,code_size, predict_terms, terms=4, units=256, image_size=64, channels=3):\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.code_size = code_size\n",
    "        self.predict_terms = predict_terms\n",
    "        self.terms = terms\n",
    "        self.units = units\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu1 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu2 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu3 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu4 = tf.keras.layers.LeakyReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense5 = tf.keras.layers.Dense(units=256, activation='linear')\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu5 = tf.keras.layers.LeakyReLU()\n",
    "        self.dense6 = tf.keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=False, name='ar_context')\n",
    "        self.linear = tf.keras.layers.Dense(predict_terms*code_size, activation='linear')    \n",
    "   \n",
    "    def encoding(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.lrelu5(x)\n",
    "        z = self.dense6(x)\n",
    "        return z\n",
    "  \n",
    "    def get_context(self, x):\n",
    "        z = self.encoding(x)\n",
    "        z = tf.reshape(z, [-1, self.terms, self.code_size])\n",
    "        c = self.gru(z)\n",
    "        return c\n",
    "    def get_prediction(self, x):\n",
    "        c = self.get_context(x)\n",
    "        z_hats = self.linear(c)\n",
    "        z_hat = tf.reshape(z_hats, [-1, self.predict_terms, self.code_size])\n",
    "        return z_hat\n",
    "\n",
    "    def optimizer(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self,weights,biases,labels,inputs,num_samples,num_classes): \n",
    "        loss = tf.nn.nce_loss(\n",
    "        weights, biases, labels, inputs, num_sampled, num_classes, num_true=1,\n",
    "        sampled_values=None, remove_accidental_hits=False, name='nce_loss')\n",
    "        return loss\n",
    "  \n",
    "    def call(self,inputs):\n",
    "        x_tm, x_tp = inputs\n",
    "        x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels])\n",
    "        x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels])\n",
    "        z_hat = self.get_prediction(x_tm)\n",
    "        z_tp = self.encoding(x_tp)\n",
    "        z_tp = tf.reshape(z_tp, [-1, self.predict_terms, self.code_size])\n",
    "        dot_prods = tf.reduce_mean(tf.reduce_mean(z_hat*z_tp, axis=-1), axis=-1, keepdims=True)\n",
    "        probs = tf.sigmoid(dot_prods)\n",
    "        return probs\n",
    "\n",
    "\n",
    "  # def save(self):\n",
    "  #       f1 = os.path.join(folder,'target_actor')\n",
    "  #       f2 = os.path.join(folder, 'target_critic')\n",
    "  #       f3 = os.path.join(folder, 'actor')\n",
    "  #       f4 = os.path.join(folder, 'critic')\n",
    "  #       self.target_actor.save(f1)\n",
    "  #       self.target_critic.save(f2)\n",
    "  #       self.actor.save(f3)\n",
    "  #       self.critic.save(f4)\n",
    "\n",
    "\n",
    "  # def load(self):\n",
    "  #   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wW6bj9SJkd20"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,state_space,action_space,capacity,batch):\n",
    "        self.capacity = capacity\n",
    "        self.batch = batch\n",
    "        \n",
    "        self.avaliable_batch = 0\n",
    "        self.idx = 0\n",
    "        self.entries = 0 \n",
    "        \n",
    "        self.states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.next_states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.actions = np.empty((self.capacity,action_space),dtype = np.float32)\n",
    "        self.rewards = np.empty((self.capacity,1),dtype = np.float32)\n",
    "        self.not_dones = np.empty((self.capacity, 1), dtype=np.float32)\n",
    "        \n",
    "    def add(self,state,next_state,action,reward,done):\n",
    "        np.copyto(self.states[self.idx], state)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_states[self.idx], next_state)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "        self.avaliable_batch= (self.avaliable_batch + 1) if self.avaliable_batch < self.batch else self.batch\n",
    "        self.entries = (self.entries + 1) if self.entries < self.capacity else self.capacity\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        \n",
    "    def sample(self):\n",
    "        num = self.avaliable_batch\n",
    "        if(num > self.batch):\n",
    "            num = self.batch\n",
    "        print('avaliable_batch: ',self.avaliable_batch, \"entries: \", self.entries,'capacity: ', self.capacity)\n",
    "        idx = np.random.choice(self.avaliable_batch,size = num,replace=False)\n",
    "        \n",
    "        states = tf.convert_to_tensor(self.states[idx])\n",
    "        next_states = tf.convert_to_tensor(self.next_states[idx])\n",
    "        actions = tf.convert_to_tensor(self.actions[idx])\n",
    "        rewards = tf.convert_to_tensor(self.rewards[idx])\n",
    "        not_dones = tf.convert_to_tensor(self.not_dones[idx])\n",
    "        \n",
    "        return states,next_states,actions,rewards,not_dones\n",
    "        \n",
    "        \n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,action_space,critic,actor_lr = 0.001):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        #self.critic = critics\n",
    "        \n",
    "        #optimizer\n",
    "        self.opt = tf.keras.optimizers.Adam(actor_lr)\n",
    "        self.critic = critic\n",
    "       \n",
    "        #model\n",
    "        self.dense1 = tf.keras.layers.Dense(400,activation = 'relu',dtype='float32')\n",
    "        self.dense2 = tf.keras.layers.Dense(300,activation='relu',dtype='float32')\n",
    "        self.dense3 = tf.keras.layers.Dense(action_space,activation = 'tanh',dtype='float32')    \n",
    "        \n",
    "    def loss(self,states,actions):\n",
    "        actions = self(states)\n",
    "        #stateactions = tf.concat([states,actions],-1)\n",
    "        Q = self.critic(states,actions)\n",
    "        loss = - tf.reduce_mean(Q)\n",
    "        return loss\n",
    "    \n",
    "    def update(self,states,actions):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(state,state)\n",
    "\n",
    "        grad = tape.gradient(loss,self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        #print('actor loss: ', loss ,\"\\n\" )\n",
    "        return loss\n",
    "    \n",
    "    def call(self,x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,critic_lr = 0.001):\n",
    "        super(Critic,self).__init__()\n",
    "        \n",
    "        # optimizer\n",
    "        self.opt = tf.keras.optimizers.Adam(critic_lr)\n",
    "        \n",
    "        # loss\n",
    "        #self.loss = tf.keras.losses.MSE\n",
    "        \n",
    "        \n",
    "        # layers\n",
    "        self.dense1 = tf.keras.layers.Dense(400,activation = 'relu',dtype='float32')\n",
    "        self.dense2 = tf.keras.layers.Dense(300,activation='relu',dtype='float32')\n",
    "        self.dense3 = tf.keras.layers.Dense(1,dtype='float32') \n",
    "        \n",
    "    #loss\n",
    "    def loss(self,actual,pred):\n",
    "        return tf.keras.losses.MSE(actual,pred)\n",
    "    \n",
    "    def update(self,states,actions,Q_h):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q = self.call(states,actions)\n",
    "            loss = self.loss(Q,Q_h)\n",
    "\n",
    "        grad = tape.gradient(loss,self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        #print('critic loss: ', loss ,\"\\n\" )\n",
    "        return loss\n",
    "    \n",
    "    #predict\n",
    "    def call(self,states,actions):\n",
    "        x = tf.concat([states,actions],-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SAC(tf.keras.Model):\n",
    "    def __init__(self,state_space,action_space,capacity = 100,batch = 1, tau=0.999,gamma=0.9,actor_lr = 0.01, critic_lr = 0.001):\n",
    "        super(SAC,self).__init__()\n",
    "        # tensorboard callbacks\n",
    "        self.cb = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
    "                   tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
    "                                          monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
    "                   tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
    "                   tf.keras.callbacks.TensorBoard()]\n",
    "        \n",
    "        \n",
    "        #hyperparameters\n",
    "        self.batch = batch\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        \n",
    "        \n",
    "        #spaces\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.state_action_space = self.action_space + self.state_space\n",
    "        \n",
    "        # replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.state_space,self.action_space,capacity,self.batch)\n",
    "        \n",
    "        # models\n",
    "        self.critic = Critic(critic_lr = critic_lr)        \n",
    "        self.actor = Actor(self.action_space,actor_lr=actor_lr,critic = self.critic)\n",
    "        self.critic.compile(optimizer = self.critic.opt,loss = self.critic.loss)\n",
    "        self.actor.compile(optimizer = self.actor.opt,loss = self.actor.loss)\n",
    "        \n",
    "        # target models\n",
    "        self.target_actor = Actor(self.action_space,actor_lr=actor_lr,critic = self.critic)\n",
    "        self.target_critic = Critic(critic_lr = critic_lr)  \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        #cpc\n",
    "        #self.cpc = CPC(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "    \n",
    "    def store_replay(self,state,next_state,action,reward,done):\n",
    "        self.replay_buffer.add(state,next_state,action,reward,done)\n",
    "    \n",
    "    def set_labels(self,states,new_states,actions,rewards):\n",
    "        mu = self.target_actor(new_states)\n",
    "        #print(mu,states)\n",
    "#         stateactions = tf.concat([states,mu],1)\n",
    "        self.Q_h = self.target_critic(new_states,mu)\n",
    "        y = rewards + self.gamma * self.Q_h\n",
    "        #y = np.concatenate(self.y,0).astype('float32') #.reshape((self.minibatch_size,1,1,1))\n",
    "        #print('y: ',self.y)\n",
    "        y = tf.reshape(y,(self.replay_buffer.avaliable_batch,1,1,1))\n",
    "        return y \n",
    "    \n",
    "    def update_target_weights(self):   \n",
    "        tgt_critic_weight = self.target_critic.get_weights()\n",
    "        tgt_actor_weight = self.target_actor.get_weights()\n",
    "        actor_weight = self.actor.get_weights()\n",
    "        critic_weight = self.target_actor.get_weights()\n",
    "        \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_critic_weight,critic_weight)):\n",
    "            tgt_critic_weight[idx] = self.tau*part_tgt + (1-self.tau)*part_net\n",
    "        \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_actor_weight,actor_weight)):\n",
    "            tgt_actor_weight[idx] = self.tau*part_tgt + (1-self.tau)*part_net\n",
    "            \n",
    "    def save(self,filename):\n",
    "        pass\n",
    "    \n",
    "    def load(self,filename):\n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StY2xL3E73ea"
   },
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, batch_size, terms, predict_terms=1, image_size=64, color=False, rescale=True, aug=True, is_training=True, method='cpc'):\n",
    "        self.batch_size = batch_size\n",
    "        self.terms = terms\n",
    "        self.predict_terms = predict_terms\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "        self.aug = aug\n",
    "        self.is_training = is_training\n",
    "        self.method = method\n",
    "        self.lena = cv2.imread(os.path.join(base_dir,'lena.jpg'))\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        if self.is_training:\n",
    "            self.x = x_train/255.0\n",
    "            self.y = y_train\n",
    "        else:\n",
    "            self.x = x_test/255.0\n",
    "            self.y = y_test\n",
    "        self.idxs = []\n",
    "        for i in range(10):\n",
    "            y = y_train if self.is_training else y_test\n",
    "            self.idxs.append(np.where(y == i)[0])\n",
    "        self.n_samples = len(self.y)//terms if self.method == 'cpc' else len(self.y)\n",
    "        self.shape = self.x.shape\n",
    "        self.n_batches = self.n_samples//batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.cpc_batch() if self.method == 'cpc' else self.benchmark_batch()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def cpc_batch(self):\n",
    "        img_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
    "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
    "        for bi in range(self.batch_size):\n",
    "            seed = np.random.randint(10)\n",
    "            sentence = np.arange(seed, seed + self.terms + self.predict_terms) % 10\n",
    "            if bi < self.batch_size//2:\n",
    "                num = np.arange(10)\n",
    "                predicted = sentence[-self.predict_terms:]\n",
    "                for i, p in enumerate(predicted):\n",
    "                    predicted[i] = np.random.choice(num[num != p], 1)\n",
    "                sentence[-self.predict_terms:] = predicted % 10\n",
    "                sentence_labels[bi, :] = 0\n",
    "            img_labels[bi, :] = sentence\n",
    "        images = self.get_samples(img_labels).reshape((self.batch_size, self.terms+self.predict_terms, self.image_size, self.image_size, 3))\n",
    "        x_images = images[:, :-self.predict_terms, ...]\n",
    "        y_images = images[:, -self.predict_terms:, ...]\n",
    "        idx = np.random.choice(self.batch_size, self.batch_size, replace=False)\n",
    "        return [x_images[idx], y_images[idx]], sentence_labels[idx]\n",
    "\n",
    "    def get_samples(self, img_labels):\n",
    "        idx = []\n",
    "        for label in img_labels.flatten():\n",
    "            idx.append(np.random.choice(self.idxs[int(label)], 1)[0])\n",
    "        img_batch = self.x[idx, :, :]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        return img_batch\n",
    "\n",
    "    def _aug_batch(self, img_batch):\n",
    "        if self.image_size != 28:\n",
    "            resized = []\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                resized.append(cv2.resize(img_batch[i], (self.image_size, self.image_size)))\n",
    "            img_batch = np.stack(resized)\n",
    "        img_batch = img_batch.reshape((img_batch.shape[0], 1, self.image_size, self.image_size))\n",
    "        img_batch = np.concatenate([img_batch, img_batch, img_batch], axis=1)\n",
    "\n",
    "        if self.color:\n",
    "            img_batch[img_batch >= 0.5] = 1\n",
    "            img_batch[img_batch < 0.5] = 0\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                x_c = np.random.randint(0, self.lena.shape[0] - self.image_size)\n",
    "                y_c = np.random.randint(0, self.lena.shape[1] - self.image_size)\n",
    "                img = self.lena[x_c:x_c+self.image_size, y_c:y_c+self.image_size]\n",
    "                img = np.array(img).transpose((2, 0, 1))/255.0\n",
    "                for j in range(3):\n",
    "                    img[j, :, :] = (img[j, :, :] + np.random.uniform(0, 1))/2.0\n",
    "                img[img_batch[i, :, :, :] == 1] = 1 - img[img_batch[i, :, :, :] == 1]\n",
    "                img_batch[i, :, :, :] = img\n",
    "\n",
    "        if self.rescale:\n",
    "            img_batch = img_batch * 2 - 1\n",
    "        img_batch = img_batch.transpose((0, 2, 3, 1))\n",
    "        return img_batch\n",
    "\n",
    "    def benchmark_batch(self):\n",
    "        idx = np.random.choice(len(self.x), self.batch_size, replace=False)\n",
    "        img_batch = self.x[idx]\n",
    "        label_batch = self.y[idx]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        label_batch = label_batch.reshape((-1, 1))\n",
    "        return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TbyX3qlk1b4g",
    "outputId": "5ee8316c-0a96-4277-b003-e9937032d726"
   },
   "outputs": [],
   "source": [
    "# #train loop\n",
    "# dh_train = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=True, method='cpc')\n",
    "# dh_test = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=False, method='cpc')\n",
    "# accuracy_metric_train = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_train = tf.keras.metrics.BinaryCrossentropy()\n",
    "# accuracy_metric_test = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_test = tf.keras.metrics.BinaryCrossentropy()\n",
    "# cpc = CPCModel(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "# optim = tf.keras.optimizers.Adam(1e-3)\n",
    "# cb = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
    "#       tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
    "#                                           monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
    "#       tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
    "#       tf.keras.callbacks.TensorBoard()]\n",
    "# cpc.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "# cpc.fit(x=dh_train, epochs=10, validation_data=dh_test, steps_per_epoch=60000//64, validation_steps=10000//64, callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vn5Hapce1cE5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtara\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# train loop params\n",
    "\n",
    "\n",
    "episodes = 1\n",
    "episode_steps = 200\n",
    "\n",
    "# pybullet setup\n",
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "env.render(mode = 'human')\n",
    "env._max_episode_steps = episode_steps\n",
    "env = gym.wrappers.Monitor(env, \"baseline_training\", video_callable=lambda episode: True, force=\"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 6\n"
     ]
    }
   ],
   "source": [
    "#network batch\n",
    "batch = 16\n",
    "\n",
    "\n",
    "#get spaces\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "print(state_space,action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0eguufNR5p4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  0  :episode:  0\n",
      "avaliable_batch:  1 entries:  1 capacity:  1000\n",
      "t:  1  :episode:  0\n",
      "avaliable_batch:  2 entries:  2 capacity:  1000\n",
      "t:  2  :episode:  0\n",
      "avaliable_batch:  3 entries:  3 capacity:  1000\n",
      "t:  3  :episode:  0\n",
      "avaliable_batch:  4 entries:  4 capacity:  1000\n",
      "t:  4  :episode:  0\n",
      "avaliable_batch:  5 entries:  5 capacity:  1000\n",
      "t:  5  :episode:  0\n",
      "avaliable_batch:  6 entries:  6 capacity:  1000\n",
      "t:  6  :episode:  0\n",
      "avaliable_batch:  7 entries:  7 capacity:  1000\n",
      "t:  7  :episode:  0\n",
      "avaliable_batch:  8 entries:  8 capacity:  1000\n",
      "t:  8  :episode:  0\n",
      "avaliable_batch:  9 entries:  9 capacity:  1000\n",
      "t:  9  :episode:  0\n",
      "avaliable_batch:  10 entries:  10 capacity:  1000\n",
      "t:  10  :episode:  0\n",
      "avaliable_batch:  11 entries:  11 capacity:  1000\n",
      "t:  11  :episode:  0\n",
      "avaliable_batch:  12 entries:  12 capacity:  1000\n",
      "t:  12  :episode:  0\n",
      "avaliable_batch:  13 entries:  13 capacity:  1000\n",
      "t:  13  :episode:  0\n",
      "avaliable_batch:  14 entries:  14 capacity:  1000\n",
      "t:  14  :episode:  0\n",
      "avaliable_batch:  15 entries:  15 capacity:  1000\n",
      "t:  15  :episode:  0\n",
      "avaliable_batch:  16 entries:  16 capacity:  1000\n",
      "t:  16  :episode:  0\n",
      "avaliable_batch:  17 entries:  17 capacity:  1000\n",
      "t:  17  :episode:  0\n",
      "avaliable_batch:  18 entries:  18 capacity:  1000\n",
      "t:  18  :episode:  0\n",
      "avaliable_batch:  19 entries:  19 capacity:  1000\n",
      "t:  19  :episode:  0\n",
      "avaliable_batch:  20 entries:  20 capacity:  1000\n",
      "t:  20  :episode:  0\n",
      "avaliable_batch:  21 entries:  21 capacity:  1000\n",
      "t:  21  :episode:  0\n",
      "avaliable_batch:  22 entries:  22 capacity:  1000\n",
      "t:  22  :episode:  0\n",
      "avaliable_batch:  23 entries:  23 capacity:  1000\n",
      "t:  23  :episode:  0\n",
      "avaliable_batch:  24 entries:  24 capacity:  1000\n",
      "t:  24  :episode:  0\n",
      "avaliable_batch:  25 entries:  25 capacity:  1000\n",
      "t:  25  :episode:  0\n",
      "avaliable_batch:  26 entries:  26 capacity:  1000\n",
      "t:  26  :episode:  0\n",
      "avaliable_batch:  27 entries:  27 capacity:  1000\n",
      "t:  27  :episode:  0\n",
      "avaliable_batch:  28 entries:  28 capacity:  1000\n",
      "t:  28  :episode:  0\n",
      "avaliable_batch:  29 entries:  29 capacity:  1000\n",
      "t:  29  :episode:  0\n",
      "avaliable_batch:  30 entries:  30 capacity:  1000\n",
      "t:  30  :episode:  0\n",
      "avaliable_batch:  31 entries:  31 capacity:  1000\n",
      "t:  31  :episode:  0\n",
      "avaliable_batch:  32 entries:  32 capacity:  1000\n",
      "t:  32  :episode:  0\n",
      "avaliable_batch:  32 entries:  33 capacity:  1000\n",
      "t:  33  :episode:  0\n",
      "avaliable_batch:  32 entries:  34 capacity:  1000\n",
      "t:  34  :episode:  0\n",
      "avaliable_batch:  32 entries:  35 capacity:  1000\n",
      "t:  35  :episode:  0\n",
      "avaliable_batch:  32 entries:  36 capacity:  1000\n",
      "t:  36  :episode:  0\n",
      "avaliable_batch:  32 entries:  37 capacity:  1000\n",
      "t:  37  :episode:  0\n",
      "avaliable_batch:  32 entries:  38 capacity:  1000\n",
      "t:  38  :episode:  0\n",
      "avaliable_batch:  32 entries:  39 capacity:  1000\n",
      "t:  39  :episode:  0\n",
      "avaliable_batch:  32 entries:  40 capacity:  1000\n",
      "t:  40  :episode:  0\n",
      "avaliable_batch:  32 entries:  41 capacity:  1000\n",
      "t:  41  :episode:  0\n",
      "avaliable_batch:  32 entries:  42 capacity:  1000\n",
      "t:  42  :episode:  0\n",
      "avaliable_batch:  32 entries:  43 capacity:  1000\n",
      "t:  43  :episode:  0\n",
      "avaliable_batch:  32 entries:  44 capacity:  1000\n",
      "t:  44  :episode:  0\n",
      "avaliable_batch:  32 entries:  45 capacity:  1000\n",
      "t:  45  :episode:  0\n",
      "avaliable_batch:  32 entries:  46 capacity:  1000\n",
      "t:  46  :episode:  0\n",
      "avaliable_batch:  32 entries:  47 capacity:  1000\n",
      "t:  47  :episode:  0\n",
      "avaliable_batch:  32 entries:  48 capacity:  1000\n",
      "t:  48  :episode:  0\n",
      "avaliable_batch:  32 entries:  49 capacity:  1000\n",
      "t:  49  :episode:  0\n",
      "avaliable_batch:  32 entries:  50 capacity:  1000\n",
      "t:  50  :episode:  0\n",
      "avaliable_batch:  32 entries:  51 capacity:  1000\n",
      "t:  51  :episode:  0\n",
      "avaliable_batch:  32 entries:  52 capacity:  1000\n",
      "t:  52  :episode:  0\n",
      "avaliable_batch:  32 entries:  53 capacity:  1000\n",
      "t:  53  :episode:  0\n",
      "avaliable_batch:  32 entries:  54 capacity:  1000\n",
      "t:  54  :episode:  0\n",
      "avaliable_batch:  32 entries:  55 capacity:  1000\n",
      "t:  55  :episode:  0\n",
      "avaliable_batch:  32 entries:  56 capacity:  1000\n",
      "t:  56  :episode:  0\n",
      "avaliable_batch:  32 entries:  57 capacity:  1000\n",
      "t:  57  :episode:  0\n",
      "avaliable_batch:  32 entries:  58 capacity:  1000\n",
      "t:  58  :episode:  0\n",
      "avaliable_batch:  32 entries:  59 capacity:  1000\n",
      "t:  59  :episode:  0\n",
      "avaliable_batch:  32 entries:  60 capacity:  1000\n",
      "t:  60  :episode:  0\n",
      "avaliable_batch:  32 entries:  61 capacity:  1000\n",
      "t:  61  :episode:  0\n",
      "avaliable_batch:  32 entries:  62 capacity:  1000\n",
      "t:  62  :episode:  0\n",
      "avaliable_batch:  32 entries:  63 capacity:  1000\n",
      "t:  63  :episode:  0\n",
      "avaliable_batch:  32 entries:  64 capacity:  1000\n",
      "t:  64  :episode:  0\n",
      "avaliable_batch:  32 entries:  65 capacity:  1000\n",
      "t:  65  :episode:  0\n",
      "avaliable_batch:  32 entries:  66 capacity:  1000\n",
      "t:  66  :episode:  0\n",
      "avaliable_batch:  32 entries:  67 capacity:  1000\n",
      "t:  67  :episode:  0\n",
      "avaliable_batch:  32 entries:  68 capacity:  1000\n",
      "t:  68  :episode:  0\n",
      "avaliable_batch:  32 entries:  69 capacity:  1000\n",
      "t:  69  :episode:  0\n",
      "avaliable_batch:  32 entries:  70 capacity:  1000\n",
      "t:  70  :episode:  0\n",
      "avaliable_batch:  32 entries:  71 capacity:  1000\n",
      "t:  71  :episode:  0\n",
      "avaliable_batch:  32 entries:  72 capacity:  1000\n",
      "t:  72  :episode:  0\n",
      "avaliable_batch:  32 entries:  73 capacity:  1000\n",
      "t:  73  :episode:  0\n",
      "avaliable_batch:  32 entries:  74 capacity:  1000\n",
      "t:  74  :episode:  0\n",
      "avaliable_batch:  32 entries:  75 capacity:  1000\n",
      "t:  75  :episode:  0\n",
      "avaliable_batch:  32 entries:  76 capacity:  1000\n",
      "t:  76  :episode:  0\n",
      "avaliable_batch:  32 entries:  77 capacity:  1000\n",
      "t:  77  :episode:  0\n",
      "avaliable_batch:  32 entries:  78 capacity:  1000\n",
      "t:  78  :episode:  0\n",
      "avaliable_batch:  32 entries:  79 capacity:  1000\n",
      "t:  79  :episode:  0\n",
      "avaliable_batch:  32 entries:  80 capacity:  1000\n",
      "t:  80  :episode:  0\n",
      "avaliable_batch:  32 entries:  81 capacity:  1000\n",
      "t:  81  :episode:  0\n",
      "avaliable_batch:  32 entries:  82 capacity:  1000\n",
      "t:  82  :episode:  0\n",
      "avaliable_batch:  32 entries:  83 capacity:  1000\n",
      "t:  83  :episode:  0\n",
      "avaliable_batch:  32 entries:  84 capacity:  1000\n",
      "t:  84  :episode:  0\n",
      "avaliable_batch:  32 entries:  85 capacity:  1000\n",
      "t:  85  :episode:  0\n",
      "avaliable_batch:  32 entries:  86 capacity:  1000\n",
      "t:  86  :episode:  0\n",
      "avaliable_batch:  32 entries:  87 capacity:  1000\n",
      "t:  87  :episode:  0\n",
      "avaliable_batch:  32 entries:  88 capacity:  1000\n",
      "t:  88  :episode:  0\n",
      "avaliable_batch:  32 entries:  89 capacity:  1000\n",
      "t:  89  :episode:  0\n",
      "avaliable_batch:  32 entries:  90 capacity:  1000\n",
      "t:  90  :episode:  0\n",
      "avaliable_batch:  32 entries:  91 capacity:  1000\n",
      "t:  91  :episode:  0\n",
      "avaliable_batch:  32 entries:  92 capacity:  1000\n",
      "t:  92  :episode:  0\n",
      "avaliable_batch:  32 entries:  93 capacity:  1000\n",
      "t:  93  :episode:  0\n",
      "avaliable_batch:  32 entries:  94 capacity:  1000\n",
      "t:  94  :episode:  0\n",
      "avaliable_batch:  32 entries:  95 capacity:  1000\n",
      "t:  95  :episode:  0\n",
      "avaliable_batch:  32 entries:  96 capacity:  1000\n",
      "t:  96  :episode:  0\n",
      "avaliable_batch:  32 entries:  97 capacity:  1000\n",
      "t:  97  :episode:  0\n",
      "avaliable_batch:  32 entries:  98 capacity:  1000\n",
      "t:  98  :episode:  0\n",
      "avaliable_batch:  32 entries:  99 capacity:  1000\n",
      "t:  99  :episode:  0\n",
      "avaliable_batch:  32 entries:  100 capacity:  1000\n",
      "t:  100  :episode:  0\n",
      "avaliable_batch:  32 entries:  101 capacity:  1000\n",
      "t:  101  :episode:  0\n",
      "avaliable_batch:  32 entries:  102 capacity:  1000\n",
      "t:  102  :episode:  0\n",
      "avaliable_batch:  32 entries:  103 capacity:  1000\n",
      "t:  103  :episode:  0\n",
      "avaliable_batch:  32 entries:  104 capacity:  1000\n",
      "t:  104  :episode:  0\n",
      "avaliable_batch:  32 entries:  105 capacity:  1000\n",
      "t:  105  :episode:  0\n",
      "avaliable_batch:  32 entries:  106 capacity:  1000\n",
      "t:  106  :episode:  0\n",
      "avaliable_batch:  32 entries:  107 capacity:  1000\n",
      "t:  107  :episode:  0\n",
      "avaliable_batch:  32 entries:  108 capacity:  1000\n",
      "t:  108  :episode:  0\n",
      "avaliable_batch:  32 entries:  109 capacity:  1000\n",
      "t:  109  :episode:  0\n",
      "avaliable_batch:  32 entries:  110 capacity:  1000\n",
      "t:  110  :episode:  0\n",
      "avaliable_batch:  32 entries:  111 capacity:  1000\n",
      "t:  111  :episode:  0\n",
      "avaliable_batch:  32 entries:  112 capacity:  1000\n",
      "t:  112  :episode:  0\n",
      "avaliable_batch:  32 entries:  113 capacity:  1000\n",
      "t:  113  :episode:  0\n",
      "avaliable_batch:  32 entries:  114 capacity:  1000\n",
      "t:  114  :episode:  0\n",
      "avaliable_batch:  32 entries:  115 capacity:  1000\n",
      "t:  115  :episode:  0\n",
      "avaliable_batch:  32 entries:  116 capacity:  1000\n",
      "t:  116  :episode:  0\n",
      "avaliable_batch:  32 entries:  117 capacity:  1000\n",
      "t:  117  :episode:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avaliable_batch:  32 entries:  118 capacity:  1000\n",
      "t:  118  :episode:  0\n",
      "avaliable_batch:  32 entries:  119 capacity:  1000\n",
      "t:  119  :episode:  0\n",
      "avaliable_batch:  32 entries:  120 capacity:  1000\n",
      "t:  120  :episode:  0\n",
      "avaliable_batch:  32 entries:  121 capacity:  1000\n",
      "t:  121  :episode:  0\n",
      "avaliable_batch:  32 entries:  122 capacity:  1000\n",
      "t:  122  :episode:  0\n",
      "avaliable_batch:  32 entries:  123 capacity:  1000\n",
      "t:  123  :episode:  0\n",
      "avaliable_batch:  32 entries:  124 capacity:  1000\n",
      "t:  124  :episode:  0\n",
      "avaliable_batch:  32 entries:  125 capacity:  1000\n",
      "t:  125  :episode:  0\n",
      "avaliable_batch:  32 entries:  126 capacity:  1000\n",
      "t:  126  :episode:  0\n",
      "avaliable_batch:  32 entries:  127 capacity:  1000\n",
      "t:  127  :episode:  0\n",
      "avaliable_batch:  32 entries:  128 capacity:  1000\n",
      "t:  128  :episode:  0\n",
      "avaliable_batch:  32 entries:  129 capacity:  1000\n",
      "t:  129  :episode:  0\n",
      "avaliable_batch:  32 entries:  130 capacity:  1000\n",
      "t:  130  :episode:  0\n",
      "avaliable_batch:  32 entries:  131 capacity:  1000\n",
      "t:  131  :episode:  0\n",
      "avaliable_batch:  32 entries:  132 capacity:  1000\n",
      "t:  132  :episode:  0\n",
      "avaliable_batch:  32 entries:  133 capacity:  1000\n",
      "t:  133  :episode:  0\n",
      "avaliable_batch:  32 entries:  134 capacity:  1000\n",
      "t:  134  :episode:  0\n",
      "avaliable_batch:  32 entries:  135 capacity:  1000\n",
      "t:  135  :episode:  0\n",
      "avaliable_batch:  32 entries:  136 capacity:  1000\n",
      "t:  136  :episode:  0\n",
      "avaliable_batch:  32 entries:  137 capacity:  1000\n",
      "t:  137  :episode:  0\n",
      "avaliable_batch:  32 entries:  138 capacity:  1000\n",
      "t:  138  :episode:  0\n",
      "avaliable_batch:  32 entries:  139 capacity:  1000\n",
      "t:  139  :episode:  0\n",
      "avaliable_batch:  32 entries:  140 capacity:  1000\n",
      "t:  140  :episode:  0\n",
      "avaliable_batch:  32 entries:  141 capacity:  1000\n",
      "t:  141  :episode:  0\n",
      "avaliable_batch:  32 entries:  142 capacity:  1000\n",
      "t:  142  :episode:  0\n",
      "avaliable_batch:  32 entries:  143 capacity:  1000\n",
      "t:  143  :episode:  0\n",
      "avaliable_batch:  32 entries:  144 capacity:  1000\n",
      "t:  144  :episode:  0\n",
      "avaliable_batch:  32 entries:  145 capacity:  1000\n",
      "t:  145  :episode:  0\n",
      "avaliable_batch:  32 entries:  146 capacity:  1000\n",
      "t:  146  :episode:  0\n",
      "avaliable_batch:  32 entries:  147 capacity:  1000\n",
      "t:  147  :episode:  0\n",
      "avaliable_batch:  32 entries:  148 capacity:  1000\n",
      "t:  148  :episode:  0\n",
      "avaliable_batch:  32 entries:  149 capacity:  1000\n",
      "t:  149  :episode:  0\n",
      "avaliable_batch:  32 entries:  150 capacity:  1000\n",
      "t:  150  :episode:  0\n",
      "avaliable_batch:  32 entries:  151 capacity:  1000\n",
      "t:  151  :episode:  0\n",
      "avaliable_batch:  32 entries:  152 capacity:  1000\n",
      "t:  152  :episode:  0\n",
      "avaliable_batch:  32 entries:  153 capacity:  1000\n",
      "t:  153  :episode:  0\n",
      "avaliable_batch:  32 entries:  154 capacity:  1000\n",
      "t:  154  :episode:  0\n",
      "avaliable_batch:  32 entries:  155 capacity:  1000\n",
      "t:  155  :episode:  0\n",
      "avaliable_batch:  32 entries:  156 capacity:  1000\n",
      "t:  156  :episode:  0\n",
      "avaliable_batch:  32 entries:  157 capacity:  1000\n",
      "t:  157  :episode:  0\n",
      "avaliable_batch:  32 entries:  158 capacity:  1000\n",
      "t:  158  :episode:  0\n",
      "avaliable_batch:  32 entries:  159 capacity:  1000\n",
      "t:  159  :episode:  0\n",
      "avaliable_batch:  32 entries:  160 capacity:  1000\n",
      "t:  160  :episode:  0\n",
      "avaliable_batch:  32 entries:  161 capacity:  1000\n",
      "t:  161  :episode:  0\n",
      "avaliable_batch:  32 entries:  162 capacity:  1000\n",
      "t:  162  :episode:  0\n",
      "avaliable_batch:  32 entries:  163 capacity:  1000\n",
      "t:  163  :episode:  0\n",
      "avaliable_batch:  32 entries:  164 capacity:  1000\n",
      "t:  164  :episode:  0\n",
      "avaliable_batch:  32 entries:  165 capacity:  1000\n",
      "t:  165  :episode:  0\n",
      "avaliable_batch:  32 entries:  166 capacity:  1000\n",
      "t:  166  :episode:  0\n",
      "avaliable_batch:  32 entries:  167 capacity:  1000\n",
      "t:  167  :episode:  0\n",
      "avaliable_batch:  32 entries:  168 capacity:  1000\n",
      "t:  168  :episode:  0\n",
      "avaliable_batch:  32 entries:  169 capacity:  1000\n",
      "t:  169  :episode:  0\n",
      "avaliable_batch:  32 entries:  170 capacity:  1000\n",
      "t:  170  :episode:  0\n",
      "avaliable_batch:  32 entries:  171 capacity:  1000\n",
      "t:  171  :episode:  0\n",
      "avaliable_batch:  32 entries:  172 capacity:  1000\n",
      "t:  172  :episode:  0\n",
      "avaliable_batch:  32 entries:  173 capacity:  1000\n",
      "t:  173  :episode:  0\n",
      "avaliable_batch:  32 entries:  174 capacity:  1000\n",
      "t:  174  :episode:  0\n",
      "avaliable_batch:  32 entries:  175 capacity:  1000\n",
      "t:  175  :episode:  0\n",
      "avaliable_batch:  32 entries:  176 capacity:  1000\n",
      "t:  176  :episode:  0\n",
      "avaliable_batch:  32 entries:  177 capacity:  1000\n",
      "t:  177  :episode:  0\n",
      "avaliable_batch:  32 entries:  178 capacity:  1000\n",
      "t:  178  :episode:  0\n",
      "avaliable_batch:  32 entries:  179 capacity:  1000\n",
      "t:  179  :episode:  0\n",
      "avaliable_batch:  32 entries:  180 capacity:  1000\n",
      "t:  180  :episode:  0\n",
      "avaliable_batch:  32 entries:  181 capacity:  1000\n",
      "t:  181  :episode:  0\n",
      "avaliable_batch:  32 entries:  182 capacity:  1000\n",
      "t:  182  :episode:  0\n",
      "avaliable_batch:  32 entries:  183 capacity:  1000\n",
      "t:  183  :episode:  0\n",
      "avaliable_batch:  32 entries:  184 capacity:  1000\n",
      "t:  184  :episode:  0\n",
      "avaliable_batch:  32 entries:  185 capacity:  1000\n",
      "t:  185  :episode:  0\n",
      "avaliable_batch:  32 entries:  186 capacity:  1000\n",
      "t:  186  :episode:  0\n",
      "avaliable_batch:  32 entries:  187 capacity:  1000\n",
      "t:  187  :episode:  0\n",
      "avaliable_batch:  32 entries:  188 capacity:  1000\n",
      "t:  188  :episode:  0\n",
      "avaliable_batch:  32 entries:  189 capacity:  1000\n",
      "t:  189  :episode:  0\n",
      "avaliable_batch:  32 entries:  190 capacity:  1000\n",
      "t:  190  :episode:  0\n",
      "avaliable_batch:  32 entries:  191 capacity:  1000\n",
      "t:  191  :episode:  0\n",
      "avaliable_batch:  32 entries:  192 capacity:  1000\n",
      "t:  192  :episode:  0\n",
      "avaliable_batch:  32 entries:  193 capacity:  1000\n",
      "t:  193  :episode:  0\n",
      "avaliable_batch:  32 entries:  194 capacity:  1000\n",
      "t:  194  :episode:  0\n",
      "avaliable_batch:  32 entries:  195 capacity:  1000\n",
      "t:  195  :episode:  0\n",
      "avaliable_batch:  32 entries:  196 capacity:  1000\n",
      "t:  196  :episode:  0\n",
      "avaliable_batch:  32 entries:  197 capacity:  1000\n",
      "t:  197  :episode:  0\n",
      "avaliable_batch:  32 entries:  198 capacity:  1000\n",
      "t:  198  :episode:  0\n",
      "avaliable_batch:  32 entries:  199 capacity:  1000\n",
      "t:  199  :episode:  0\n",
      "avaliable_batch:  32 entries:  200 capacity:  1000\n",
      "Episode 0 finished after 200 timesteps with reward -326.6349948698745\n",
      "t:  0  :episode:  1\n",
      "avaliable_batch:  32 entries:  201 capacity:  1000\n",
      "t:  1  :episode:  1\n",
      "avaliable_batch:  32 entries:  202 capacity:  1000\n",
      "t:  2  :episode:  1\n",
      "avaliable_batch:  32 entries:  203 capacity:  1000\n",
      "t:  3  :episode:  1\n",
      "avaliable_batch:  32 entries:  204 capacity:  1000\n",
      "t:  4  :episode:  1\n",
      "avaliable_batch:  32 entries:  205 capacity:  1000\n",
      "t:  5  :episode:  1\n",
      "avaliable_batch:  32 entries:  206 capacity:  1000\n",
      "t:  6  :episode:  1\n",
      "avaliable_batch:  32 entries:  207 capacity:  1000\n",
      "t:  7  :episode:  1\n",
      "avaliable_batch:  32 entries:  208 capacity:  1000\n",
      "t:  8  :episode:  1\n",
      "avaliable_batch:  32 entries:  209 capacity:  1000\n",
      "t:  9  :episode:  1\n",
      "avaliable_batch:  32 entries:  210 capacity:  1000\n",
      "t:  10  :episode:  1\n",
      "avaliable_batch:  32 entries:  211 capacity:  1000\n",
      "t:  11  :episode:  1\n",
      "avaliable_batch:  32 entries:  212 capacity:  1000\n",
      "t:  12  :episode:  1\n",
      "avaliable_batch:  32 entries:  213 capacity:  1000\n",
      "t:  13  :episode:  1\n",
      "avaliable_batch:  32 entries:  214 capacity:  1000\n",
      "t:  14  :episode:  1\n",
      "avaliable_batch:  32 entries:  215 capacity:  1000\n",
      "t:  15  :episode:  1\n",
      "avaliable_batch:  32 entries:  216 capacity:  1000\n",
      "t:  16  :episode:  1\n",
      "avaliable_batch:  32 entries:  217 capacity:  1000\n",
      "t:  17  :episode:  1\n",
      "avaliable_batch:  32 entries:  218 capacity:  1000\n",
      "t:  18  :episode:  1\n",
      "avaliable_batch:  32 entries:  219 capacity:  1000\n",
      "t:  19  :episode:  1\n",
      "avaliable_batch:  32 entries:  220 capacity:  1000\n",
      "t:  20  :episode:  1\n",
      "avaliable_batch:  32 entries:  221 capacity:  1000\n",
      "t:  21  :episode:  1\n",
      "avaliable_batch:  32 entries:  222 capacity:  1000\n",
      "t:  22  :episode:  1\n",
      "avaliable_batch:  32 entries:  223 capacity:  1000\n",
      "t:  23  :episode:  1\n",
      "avaliable_batch:  32 entries:  224 capacity:  1000\n",
      "t:  24  :episode:  1\n",
      "avaliable_batch:  32 entries:  225 capacity:  1000\n",
      "t:  25  :episode:  1\n",
      "avaliable_batch:  32 entries:  226 capacity:  1000\n",
      "t:  26  :episode:  1\n",
      "avaliable_batch:  32 entries:  227 capacity:  1000\n",
      "t:  27  :episode:  1\n",
      "avaliable_batch:  32 entries:  228 capacity:  1000\n",
      "t:  28  :episode:  1\n",
      "avaliable_batch:  32 entries:  229 capacity:  1000\n",
      "t:  29  :episode:  1\n",
      "avaliable_batch:  32 entries:  230 capacity:  1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  30  :episode:  1\n",
      "avaliable_batch:  32 entries:  231 capacity:  1000\n",
      "t:  31  :episode:  1\n",
      "avaliable_batch:  32 entries:  232 capacity:  1000\n",
      "t:  32  :episode:  1\n",
      "avaliable_batch:  32 entries:  233 capacity:  1000\n",
      "t:  33  :episode:  1\n",
      "avaliable_batch:  32 entries:  234 capacity:  1000\n",
      "t:  34  :episode:  1\n",
      "avaliable_batch:  32 entries:  235 capacity:  1000\n",
      "t:  35  :episode:  1\n",
      "avaliable_batch:  32 entries:  236 capacity:  1000\n",
      "t:  36  :episode:  1\n",
      "avaliable_batch:  32 entries:  237 capacity:  1000\n",
      "t:  37  :episode:  1\n",
      "avaliable_batch:  32 entries:  238 capacity:  1000\n",
      "t:  38  :episode:  1\n",
      "avaliable_batch:  32 entries:  239 capacity:  1000\n",
      "t:  39  :episode:  1\n",
      "avaliable_batch:  32 entries:  240 capacity:  1000\n",
      "t:  40  :episode:  1\n",
      "avaliable_batch:  32 entries:  241 capacity:  1000\n",
      "t:  41  :episode:  1\n",
      "avaliable_batch:  32 entries:  242 capacity:  1000\n",
      "t:  42  :episode:  1\n",
      "avaliable_batch:  32 entries:  243 capacity:  1000\n",
      "t:  43  :episode:  1\n",
      "avaliable_batch:  32 entries:  244 capacity:  1000\n",
      "t:  44  :episode:  1\n",
      "avaliable_batch:  32 entries:  245 capacity:  1000\n",
      "t:  45  :episode:  1\n",
      "avaliable_batch:  32 entries:  246 capacity:  1000\n",
      "t:  46  :episode:  1\n",
      "avaliable_batch:  32 entries:  247 capacity:  1000\n",
      "t:  47  :episode:  1\n",
      "avaliable_batch:  32 entries:  248 capacity:  1000\n",
      "t:  48  :episode:  1\n",
      "avaliable_batch:  32 entries:  249 capacity:  1000\n",
      "t:  49  :episode:  1\n",
      "avaliable_batch:  32 entries:  250 capacity:  1000\n",
      "t:  50  :episode:  1\n",
      "avaliable_batch:  32 entries:  251 capacity:  1000\n",
      "t:  51  :episode:  1\n",
      "avaliable_batch:  32 entries:  252 capacity:  1000\n",
      "t:  52  :episode:  1\n",
      "avaliable_batch:  32 entries:  253 capacity:  1000\n",
      "t:  53  :episode:  1\n",
      "avaliable_batch:  32 entries:  254 capacity:  1000\n",
      "t:  54  :episode:  1\n",
      "avaliable_batch:  32 entries:  255 capacity:  1000\n",
      "t:  55  :episode:  1\n",
      "avaliable_batch:  32 entries:  256 capacity:  1000\n",
      "t:  56  :episode:  1\n",
      "avaliable_batch:  32 entries:  257 capacity:  1000\n",
      "t:  57  :episode:  1\n",
      "avaliable_batch:  32 entries:  258 capacity:  1000\n",
      "t:  58  :episode:  1\n",
      "avaliable_batch:  32 entries:  259 capacity:  1000\n",
      "t:  59  :episode:  1\n",
      "avaliable_batch:  32 entries:  260 capacity:  1000\n",
      "t:  60  :episode:  1\n",
      "avaliable_batch:  32 entries:  261 capacity:  1000\n",
      "t:  61  :episode:  1\n",
      "avaliable_batch:  32 entries:  262 capacity:  1000\n",
      "t:  62  :episode:  1\n",
      "avaliable_batch:  32 entries:  263 capacity:  1000\n",
      "t:  63  :episode:  1\n",
      "avaliable_batch:  32 entries:  264 capacity:  1000\n",
      "t:  64  :episode:  1\n",
      "avaliable_batch:  32 entries:  265 capacity:  1000\n",
      "t:  65  :episode:  1\n",
      "avaliable_batch:  32 entries:  266 capacity:  1000\n",
      "t:  66  :episode:  1\n",
      "avaliable_batch:  32 entries:  267 capacity:  1000\n",
      "t:  67  :episode:  1\n",
      "avaliable_batch:  32 entries:  268 capacity:  1000\n",
      "t:  68  :episode:  1\n",
      "avaliable_batch:  32 entries:  269 capacity:  1000\n",
      "t:  69  :episode:  1\n",
      "avaliable_batch:  32 entries:  270 capacity:  1000\n",
      "t:  70  :episode:  1\n",
      "avaliable_batch:  32 entries:  271 capacity:  1000\n",
      "t:  71  :episode:  1\n",
      "avaliable_batch:  32 entries:  272 capacity:  1000\n",
      "t:  72  :episode:  1\n",
      "avaliable_batch:  32 entries:  273 capacity:  1000\n",
      "t:  73  :episode:  1\n",
      "avaliable_batch:  32 entries:  274 capacity:  1000\n",
      "t:  74  :episode:  1\n",
      "avaliable_batch:  32 entries:  275 capacity:  1000\n",
      "t:  75  :episode:  1\n",
      "avaliable_batch:  32 entries:  276 capacity:  1000\n",
      "t:  76  :episode:  1\n",
      "avaliable_batch:  32 entries:  277 capacity:  1000\n",
      "t:  77  :episode:  1\n",
      "avaliable_batch:  32 entries:  278 capacity:  1000\n",
      "t:  78  :episode:  1\n",
      "avaliable_batch:  32 entries:  279 capacity:  1000\n",
      "t:  79  :episode:  1\n",
      "avaliable_batch:  32 entries:  280 capacity:  1000\n",
      "t:  80  :episode:  1\n",
      "avaliable_batch:  32 entries:  281 capacity:  1000\n",
      "t:  81  :episode:  1\n",
      "avaliable_batch:  32 entries:  282 capacity:  1000\n",
      "t:  82  :episode:  1\n",
      "avaliable_batch:  32 entries:  283 capacity:  1000\n",
      "t:  83  :episode:  1\n",
      "avaliable_batch:  32 entries:  284 capacity:  1000\n",
      "t:  84  :episode:  1\n",
      "avaliable_batch:  32 entries:  285 capacity:  1000\n",
      "t:  85  :episode:  1\n",
      "avaliable_batch:  32 entries:  286 capacity:  1000\n",
      "t:  86  :episode:  1\n",
      "avaliable_batch:  32 entries:  287 capacity:  1000\n",
      "t:  87  :episode:  1\n",
      "avaliable_batch:  32 entries:  288 capacity:  1000\n",
      "t:  88  :episode:  1\n",
      "avaliable_batch:  32 entries:  289 capacity:  1000\n",
      "t:  89  :episode:  1\n",
      "avaliable_batch:  32 entries:  290 capacity:  1000\n",
      "t:  90  :episode:  1\n",
      "avaliable_batch:  32 entries:  291 capacity:  1000\n",
      "t:  91  :episode:  1\n",
      "avaliable_batch:  32 entries:  292 capacity:  1000\n",
      "t:  92  :episode:  1\n",
      "avaliable_batch:  32 entries:  293 capacity:  1000\n",
      "t:  93  :episode:  1\n",
      "avaliable_batch:  32 entries:  294 capacity:  1000\n",
      "t:  94  :episode:  1\n",
      "avaliable_batch:  32 entries:  295 capacity:  1000\n",
      "t:  95  :episode:  1\n",
      "avaliable_batch:  32 entries:  296 capacity:  1000\n",
      "t:  96  :episode:  1\n",
      "avaliable_batch:  32 entries:  297 capacity:  1000\n",
      "t:  97  :episode:  1\n",
      "avaliable_batch:  32 entries:  298 capacity:  1000\n",
      "t:  98  :episode:  1\n",
      "avaliable_batch:  32 entries:  299 capacity:  1000\n",
      "t:  99  :episode:  1\n",
      "avaliable_batch:  32 entries:  300 capacity:  1000\n",
      "t:  100  :episode:  1\n",
      "avaliable_batch:  32 entries:  301 capacity:  1000\n",
      "t:  101  :episode:  1\n",
      "avaliable_batch:  32 entries:  302 capacity:  1000\n",
      "t:  102  :episode:  1\n",
      "avaliable_batch:  32 entries:  303 capacity:  1000\n",
      "t:  103  :episode:  1\n",
      "avaliable_batch:  32 entries:  304 capacity:  1000\n",
      "t:  104  :episode:  1\n",
      "avaliable_batch:  32 entries:  305 capacity:  1000\n",
      "t:  105  :episode:  1\n",
      "avaliable_batch:  32 entries:  306 capacity:  1000\n",
      "t:  106  :episode:  1\n",
      "avaliable_batch:  32 entries:  307 capacity:  1000\n",
      "t:  107  :episode:  1\n",
      "avaliable_batch:  32 entries:  308 capacity:  1000\n",
      "t:  108  :episode:  1\n",
      "avaliable_batch:  32 entries:  309 capacity:  1000\n",
      "t:  109  :episode:  1\n",
      "avaliable_batch:  32 entries:  310 capacity:  1000\n",
      "t:  110  :episode:  1\n",
      "avaliable_batch:  32 entries:  311 capacity:  1000\n",
      "t:  111  :episode:  1\n",
      "avaliable_batch:  32 entries:  312 capacity:  1000\n",
      "t:  112  :episode:  1\n",
      "avaliable_batch:  32 entries:  313 capacity:  1000\n",
      "t:  113  :episode:  1\n",
      "avaliable_batch:  32 entries:  314 capacity:  1000\n",
      "t:  114  :episode:  1\n",
      "avaliable_batch:  32 entries:  315 capacity:  1000\n",
      "t:  115  :episode:  1\n",
      "avaliable_batch:  32 entries:  316 capacity:  1000\n",
      "t:  116  :episode:  1\n",
      "avaliable_batch:  32 entries:  317 capacity:  1000\n",
      "t:  117  :episode:  1\n",
      "avaliable_batch:  32 entries:  318 capacity:  1000\n",
      "t:  118  :episode:  1\n",
      "avaliable_batch:  32 entries:  319 capacity:  1000\n",
      "t:  119  :episode:  1\n",
      "avaliable_batch:  32 entries:  320 capacity:  1000\n",
      "t:  120  :episode:  1\n",
      "avaliable_batch:  32 entries:  321 capacity:  1000\n",
      "t:  121  :episode:  1\n",
      "avaliable_batch:  32 entries:  322 capacity:  1000\n",
      "t:  122  :episode:  1\n",
      "avaliable_batch:  32 entries:  323 capacity:  1000\n",
      "t:  123  :episode:  1\n",
      "avaliable_batch:  32 entries:  324 capacity:  1000\n",
      "t:  124  :episode:  1\n",
      "avaliable_batch:  32 entries:  325 capacity:  1000\n",
      "t:  125  :episode:  1\n",
      "avaliable_batch:  32 entries:  326 capacity:  1000\n",
      "t:  126  :episode:  1\n",
      "avaliable_batch:  32 entries:  327 capacity:  1000\n",
      "t:  127  :episode:  1\n",
      "avaliable_batch:  32 entries:  328 capacity:  1000\n",
      "t:  128  :episode:  1\n",
      "avaliable_batch:  32 entries:  329 capacity:  1000\n",
      "t:  129  :episode:  1\n",
      "avaliable_batch:  32 entries:  330 capacity:  1000\n",
      "t:  130  :episode:  1\n",
      "avaliable_batch:  32 entries:  331 capacity:  1000\n",
      "t:  131  :episode:  1\n",
      "avaliable_batch:  32 entries:  332 capacity:  1000\n",
      "t:  132  :episode:  1\n",
      "avaliable_batch:  32 entries:  333 capacity:  1000\n",
      "t:  133  :episode:  1\n",
      "avaliable_batch:  32 entries:  334 capacity:  1000\n",
      "t:  134  :episode:  1\n",
      "avaliable_batch:  32 entries:  335 capacity:  1000\n",
      "t:  135  :episode:  1\n",
      "avaliable_batch:  32 entries:  336 capacity:  1000\n",
      "t:  136  :episode:  1\n",
      "avaliable_batch:  32 entries:  337 capacity:  1000\n",
      "t:  137  :episode:  1\n",
      "avaliable_batch:  32 entries:  338 capacity:  1000\n",
      "t:  138  :episode:  1\n",
      "avaliable_batch:  32 entries:  339 capacity:  1000\n",
      "t:  139  :episode:  1\n",
      "avaliable_batch:  32 entries:  340 capacity:  1000\n",
      "t:  140  :episode:  1\n",
      "avaliable_batch:  32 entries:  341 capacity:  1000\n",
      "t:  141  :episode:  1\n",
      "avaliable_batch:  32 entries:  342 capacity:  1000\n",
      "t:  142  :episode:  1\n",
      "avaliable_batch:  32 entries:  343 capacity:  1000\n",
      "t:  143  :episode:  1\n",
      "avaliable_batch:  32 entries:  344 capacity:  1000\n",
      "t:  144  :episode:  1\n",
      "avaliable_batch:  32 entries:  345 capacity:  1000\n",
      "t:  145  :episode:  1\n",
      "avaliable_batch:  32 entries:  346 capacity:  1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  146  :episode:  1\n",
      "avaliable_batch:  32 entries:  347 capacity:  1000\n",
      "t:  147  :episode:  1\n",
      "avaliable_batch:  32 entries:  348 capacity:  1000\n",
      "t:  148  :episode:  1\n",
      "avaliable_batch:  32 entries:  349 capacity:  1000\n",
      "t:  149  :episode:  1\n",
      "avaliable_batch:  32 entries:  350 capacity:  1000\n",
      "t:  150  :episode:  1\n",
      "avaliable_batch:  32 entries:  351 capacity:  1000\n",
      "t:  151  :episode:  1\n",
      "avaliable_batch:  32 entries:  352 capacity:  1000\n",
      "t:  152  :episode:  1\n",
      "avaliable_batch:  32 entries:  353 capacity:  1000\n",
      "t:  153  :episode:  1\n",
      "avaliable_batch:  32 entries:  354 capacity:  1000\n",
      "t:  154  :episode:  1\n",
      "avaliable_batch:  32 entries:  355 capacity:  1000\n",
      "t:  155  :episode:  1\n",
      "avaliable_batch:  32 entries:  356 capacity:  1000\n",
      "t:  156  :episode:  1\n",
      "avaliable_batch:  32 entries:  357 capacity:  1000\n",
      "t:  157  :episode:  1\n",
      "avaliable_batch:  32 entries:  358 capacity:  1000\n",
      "t:  158  :episode:  1\n",
      "avaliable_batch:  32 entries:  359 capacity:  1000\n",
      "t:  159  :episode:  1\n",
      "avaliable_batch:  32 entries:  360 capacity:  1000\n",
      "t:  160  :episode:  1\n",
      "avaliable_batch:  32 entries:  361 capacity:  1000\n",
      "t:  161  :episode:  1\n",
      "avaliable_batch:  32 entries:  362 capacity:  1000\n",
      "t:  162  :episode:  1\n",
      "avaliable_batch:  32 entries:  363 capacity:  1000\n",
      "t:  163  :episode:  1\n",
      "avaliable_batch:  32 entries:  364 capacity:  1000\n",
      "t:  164  :episode:  1\n",
      "avaliable_batch:  32 entries:  365 capacity:  1000\n",
      "t:  165  :episode:  1\n",
      "avaliable_batch:  32 entries:  366 capacity:  1000\n",
      "t:  166  :episode:  1\n",
      "avaliable_batch:  32 entries:  367 capacity:  1000\n",
      "t:  167  :episode:  1\n",
      "avaliable_batch:  32 entries:  368 capacity:  1000\n",
      "t:  168  :episode:  1\n",
      "avaliable_batch:  32 entries:  369 capacity:  1000\n",
      "t:  169  :episode:  1\n",
      "avaliable_batch:  32 entries:  370 capacity:  1000\n",
      "t:  170  :episode:  1\n",
      "avaliable_batch:  32 entries:  371 capacity:  1000\n",
      "t:  171  :episode:  1\n",
      "avaliable_batch:  32 entries:  372 capacity:  1000\n",
      "t:  172  :episode:  1\n",
      "avaliable_batch:  32 entries:  373 capacity:  1000\n",
      "t:  173  :episode:  1\n",
      "avaliable_batch:  32 entries:  374 capacity:  1000\n",
      "t:  174  :episode:  1\n",
      "avaliable_batch:  32 entries:  375 capacity:  1000\n",
      "t:  175  :episode:  1\n",
      "avaliable_batch:  32 entries:  376 capacity:  1000\n",
      "t:  176  :episode:  1\n",
      "avaliable_batch:  32 entries:  377 capacity:  1000\n",
      "t:  177  :episode:  1\n",
      "avaliable_batch:  32 entries:  378 capacity:  1000\n",
      "t:  178  :episode:  1\n",
      "avaliable_batch:  32 entries:  379 capacity:  1000\n",
      "t:  179  :episode:  1\n",
      "avaliable_batch:  32 entries:  380 capacity:  1000\n",
      "t:  180  :episode:  1\n",
      "avaliable_batch:  32 entries:  381 capacity:  1000\n",
      "t:  181  :episode:  1\n",
      "avaliable_batch:  32 entries:  382 capacity:  1000\n",
      "t:  182  :episode:  1\n",
      "avaliable_batch:  32 entries:  383 capacity:  1000\n",
      "t:  183  :episode:  1\n",
      "avaliable_batch:  32 entries:  384 capacity:  1000\n",
      "t:  184  :episode:  1\n",
      "avaliable_batch:  32 entries:  385 capacity:  1000\n",
      "t:  185  :episode:  1\n",
      "avaliable_batch:  32 entries:  386 capacity:  1000\n",
      "t:  186  :episode:  1\n",
      "avaliable_batch:  32 entries:  387 capacity:  1000\n",
      "t:  187  :episode:  1\n",
      "avaliable_batch:  32 entries:  388 capacity:  1000\n",
      "t:  188  :episode:  1\n",
      "avaliable_batch:  32 entries:  389 capacity:  1000\n",
      "t:  189  :episode:  1\n",
      "avaliable_batch:  32 entries:  390 capacity:  1000\n",
      "t:  190  :episode:  1\n",
      "avaliable_batch:  32 entries:  391 capacity:  1000\n",
      "t:  191  :episode:  1\n",
      "avaliable_batch:  32 entries:  392 capacity:  1000\n",
      "t:  192  :episode:  1\n",
      "avaliable_batch:  32 entries:  393 capacity:  1000\n",
      "t:  193  :episode:  1\n",
      "avaliable_batch:  32 entries:  394 capacity:  1000\n",
      "t:  194  :episode:  1\n",
      "avaliable_batch:  32 entries:  395 capacity:  1000\n",
      "t:  195  :episode:  1\n",
      "avaliable_batch:  32 entries:  396 capacity:  1000\n",
      "t:  196  :episode:  1\n",
      "avaliable_batch:  32 entries:  397 capacity:  1000\n",
      "t:  197  :episode:  1\n",
      "avaliable_batch:  32 entries:  398 capacity:  1000\n",
      "t:  198  :episode:  1\n",
      "avaliable_batch:  32 entries:  399 capacity:  1000\n",
      "t:  199  :episode:  1\n",
      "avaliable_batch:  32 entries:  400 capacity:  1000\n",
      "Episode 1 finished after 200 timesteps with reward -338.6890578782872\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "sac = SAC(action_space=action_space,state_space=state_space,capacity = 100,batch = 32)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    sumreward = 0\n",
    "    for step in range(episode_steps):\n",
    "        #print(observation)\n",
    "        print('t: ',step, ' :episode: ',episode )\n",
    "        #print('state: ',state)\n",
    "        \n",
    "        # get action\n",
    "        state = tf.cast(tf.reshape(state,(1,1,state_space)),dtype='float32')\n",
    "        #print(state)\n",
    "        tensor_action = sac.actor(state)\n",
    "        action = tensor_action[0][0]\n",
    "        #print('action: ',action)\n",
    "        \n",
    "        # execute action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        sumreward += reward\n",
    "\n",
    "        # store transitions\n",
    "        sac.store_replay(state,next_state,action,reward,done)\n",
    "        \n",
    "        #print('state: ',state)\n",
    "        #print('next_state: ',next_state)\n",
    "        #print('action: ',action)\n",
    "        #print('reward: ',reward)\n",
    "\n",
    "        #sample minibatch from data\n",
    "        states,next_states,actions,rewards,not_done = sac.replay_buffer.sample()\n",
    "        \n",
    "        #set labels y_i\n",
    "        y = sac.set_labels(states,next_states,actions,rewards)\n",
    "        \n",
    "        # update critic net\n",
    "        sac.critic.update(states, actions, y)\n",
    "        #losses[episode*timesteps + t] = loss\n",
    "        #losses[i_episode*timesteps+] = history.history\n",
    "        \n",
    "        #update actor net\n",
    "        sac.actor.update(states,actions)\n",
    "        #print('weight check: ',rl.actor.get_weights(),'\\n')\n",
    "        \n",
    "        #update target nets\n",
    "        sac.update_target_weights()\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #rewards[episode] = sumreward\n",
    "            sac.save('baseline')\n",
    "            print(\"Episode {} finished after {} timesteps with reward {}\".format(episode,step+1,sumreward))\n",
    "            break\n",
    "print('done') \n",
    "sac.save('baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aFHYhHwRxH6"
   },
   "source": [
    " https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss(InfoNCE Loss )\n",
    "<br>\n",
    "Representation Learning with Contrastive Predictive Coding\n",
    "<br>\n",
    "https://github.com/gdao-research/cpc/blob/master/cpc/data_handler.py (CPC)\n",
    "<br>\n",
    "https://github.com/davidtellez/contrastive-predictive-coding/blob/master/train_model.py (CPC)\n",
    "<br>\n",
    "https://github.com/MishaLaskin/curl/blob/23b0880708c29b078b0a25e62ff31fb587587b18/utils.py#L123 (replay buffer and SAC)\n",
    "<br>\n",
    "https://github.com/marload/DeepRL-TensorFlow2/blob/master/A2C/A2C_Discrete.py (A2C)\n",
    "<br>\n",
    "https://github.com/germain-hug/Deep-RL-Keras/blob/master/A3C/a3c.py (A3C)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CPCprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:csci-7000-rl] *",
   "language": "python",
   "name": "conda-env-csci-7000-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
