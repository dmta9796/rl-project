{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "z57IXf9q1Ovh",
    "outputId": "1b2cbade-88b2-48d7-b64c-3ab7d29c1715"
   },
   "outputs": [],
   "source": [
    "# if colab\n",
    "\n",
    "# !pip install pybullet\n",
    "# !pip install gym\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t8ezUpd1SIy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np \n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "import pybullet_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "y0_9dbk8Cbnv",
    "outputId": "4cdb116a-5cec-4456-dc79-f07bb48d9fe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 654765645\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# check if GPU\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hG1Ruv5FCgDv",
    "outputId": "4fe5a6b2-b4e6-4241-e5ca-c412cf4637a7"
   },
   "outputs": [],
   "source": [
    "# colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = \"drive/My Drive/\"\n",
    "# base_dir = root_dir + 'CPCtesting'\n",
    "# os.makedirs(base_dir,exist_ok=True)\n",
    "\n",
    "# train_dir = base_dir + '/train'\n",
    "# os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "# model_dir = base_dir + '/model'\n",
    "# os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "# if local machine\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "train_dir = os.path.join(base_dir , 'train')\n",
    "os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "model_dir = os.path.join(base_dir , 'model')\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "# logs_base_dir = os.path.join(base_dir , 'logs')\n",
    "\n",
    "log_dir = os.path.join(base_dir , 'training_logs_save')\n",
    "reward_dir = os.path.join(base_dir , 'training_rewards_save')\n",
    "\n",
    "#remove old logs\n",
    "fileList1 = glob.glob(os.path.join(log_dir , \"events.*\"))\n",
    "fileList2 = glob.glob(os.path.join(reward_dir , \"events.*\"))\n",
    "\n",
    "for filePath in fileList1:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except:\n",
    "        print(\"Error while deleting file : \", filePath)\n",
    "        \n",
    "for filePath in fileList2:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except:\n",
    "        print(\"Error while deleting file : \", filePath)\n",
    "\n",
    "\n",
    "# tensorboard directories\n",
    "# %load_ext tensorboard\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(reward_dir,exist_ok=True)\n",
    "# %tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWEWANi66t_w"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "# (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIKY-VyC1bsu"
   },
   "outputs": [],
   "source": [
    "class CPCModel(tf.keras.Model):\n",
    "    def __init__(self,code_size, predict_terms, terms=4, units=256, image_size=64, channels=3):\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.code_size = code_size\n",
    "        self.predict_terms = predict_terms\n",
    "        self.terms = terms\n",
    "        self.units = units\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu1 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu2 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu3 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu4 = tf.keras.layers.LeakyReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense5 = tf.keras.layers.Dense(units=256, activation='linear')\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu5 = tf.keras.layers.LeakyReLU()\n",
    "        self.dense6 = tf.keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=False, name='ar_context')\n",
    "        self.linear = tf.keras.layers.Dense(predict_terms*code_size, activation='linear')    \n",
    "   \n",
    "    def encoding(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.lrelu5(x)\n",
    "        z = self.dense6(x)\n",
    "        return z\n",
    "  \n",
    "    def get_context(self, x):\n",
    "        z = self.encoding(x)\n",
    "        z = tf.reshape(z, [-1, self.terms, self.code_size])\n",
    "        c = self.gru(z)\n",
    "        return c\n",
    "    def get_prediction(self, x):\n",
    "        c = self.get_context(x)\n",
    "        z_hats = self.linear(c)\n",
    "        z_hat = tf.reshape(z_hats, [-1, self.predict_terms, self.code_size])\n",
    "        return z_hat\n",
    "\n",
    "    def optimizer(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self,weights,biases,labels,inputs,num_samples,num_classes): \n",
    "        loss = tf.nn.nce_loss(\n",
    "        weights, biases, labels, inputs, num_sampled, num_classes, num_true=1,\n",
    "        sampled_values=None, remove_accidental_hits=False, name='nce_loss')\n",
    "        return loss\n",
    "  \n",
    "    def call(self,inputs):\n",
    "        x_tm, x_tp = inputs\n",
    "        x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels])\n",
    "        x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels])\n",
    "        z_hat = self.get_prediction(x_tm)\n",
    "        z_tp = self.encoding(x_tp)\n",
    "        z_tp = tf.reshape(z_tp, [-1, self.predict_terms, self.code_size])\n",
    "        dot_prods = tf.reduce_mean(tf.reduce_mean(z_hat*z_tp, axis=-1), axis=-1, keepdims=True)\n",
    "        probs = tf.sigmoid(dot_prods)\n",
    "        return probs\n",
    "\n",
    "\n",
    "  # def save(self):\n",
    "  #       f1 = os.path.join(folder,'target_actor')\n",
    "  #       f2 = os.path.join(folder, 'target_critic')\n",
    "  #       f3 = os.path.join(folder, 'actor')\n",
    "  #       f4 = os.path.join(folder, 'critic')\n",
    "  #       self.target_actor.save(f1)\n",
    "  #       self.target_critic.save(f2)\n",
    "  #       self.actor.save(f3)\n",
    "  #       self.critic.save(f4)\n",
    "\n",
    "\n",
    "  # def load(self):\n",
    "  #   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wW6bj9SJkd20"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,state_space,action_space,capacity,batch):\n",
    "        self.capacity = capacity\n",
    "        self.batch = batch\n",
    "        self.elements = 0\n",
    "        \n",
    "        self.avaliable_batch = 0\n",
    "        self.idx = 0\n",
    "        self.entries = 0 \n",
    "        \n",
    "        self.states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.next_states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.actions = np.empty((self.capacity,action_space),dtype = np.float32)\n",
    "        self.rewards = np.empty((self.capacity,1),dtype = np.float32)\n",
    "        self.not_dones = np.empty((self.capacity, 1), dtype=np.float32)\n",
    "        \n",
    "    def add(self,state,next_state,action,reward,done):\n",
    "        np.copyto(self.states[self.idx], state)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_states[self.idx], next_state)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "        #self.avaliable_batch= (self.avaliable_batch + 1) if self.avaliable_batch < self.batch else self.batch\n",
    "        #self.entries = (self.entries + 1) if self.entries < self.capacity else self.capacity\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.entries = np.minimum(self.entries + 1, self.capacity)\n",
    "        \n",
    "    def sample(self):\n",
    "        num = self.entries\n",
    "        if(num > self.batch):\n",
    "            num = self.batch\n",
    "        #print('avaliable_batch: ',self.avaliable_batch, \"entries: \", self.entries,'capacity: ', self.capacity)\n",
    "        idx = np.random.choice(self.entries,size = num,replace=False)\n",
    "        #print('test idx: ', idx)\n",
    "        \n",
    "        states = tf.convert_to_tensor(self.states[idx])\n",
    "        next_states = tf.convert_to_tensor(self.next_states[idx])\n",
    "        actions = tf.convert_to_tensor(self.actions[idx])\n",
    "        rewards = tf.convert_to_tensor(self.rewards[idx])\n",
    "        not_dones = tf.convert_to_tensor(self.not_dones[idx])\n",
    "        \n",
    "        return states,next_states,actions,rewards,not_dones            \n",
    "            \n",
    "        \n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,action_space):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        #params\n",
    "        self.action_space = action_space\n",
    "       \n",
    "        #model\n",
    "        self.dense1 = tf.keras.layers.Dense(400,\n",
    "                                            #input_shape = (None,1,1,state_space),\n",
    "                                            activation = 'relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed)\n",
    "                                           )\n",
    "        self.dense2 = tf.keras.layers.Dense(300,\n",
    "                                            activation='relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed)\n",
    "                                            )\n",
    "        self.dense3 = tf.keras.layers.Dense(self.action_space,\n",
    "                                            bias_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                             maxval=0.003,\n",
    "                                                                                             seed = seed\n",
    "                                                                                            ),\n",
    "                                            kernel_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                               maxval=0.003,\n",
    "                                                                                               seed = seed\n",
    "                                                                                              )\n",
    "                                           )\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic,self).__init__()\n",
    "        # layers\n",
    "        self.dense1 = tf.keras.layers.Dense(400,\n",
    "                                            #input_shape=(1,1,combined_space),\n",
    "                                            activation = 'relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                           )\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.dense2 = tf.keras.layers.Dense(300,\n",
    "                                            activation='relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed\n",
    "                                                                                                      ),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                            )\n",
    "        self.dense3 = tf.keras.layers.Dense(1,\n",
    "                                            bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            kernel_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                               maxval=0.003,\n",
    "                                                                                               seed = seed\n",
    "                                                                                              ),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                            ) \n",
    "    #predict\n",
    "    def call(self,data): #states,actions):\n",
    "        [states,actions] = data\n",
    "        #x = tf.concat([states,actions],-1)\n",
    "        y = self.dense1(states)\n",
    "        x = self.concat1([y,actions])\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SAC():\n",
    "    def __init__(self,\n",
    "                 state_space,\n",
    "                 action_space,\n",
    "                 capacity = 1000,\n",
    "                 batch = 1, \n",
    "                 tau=0.999,\n",
    "                 gamma=0.99,\n",
    "                 actor_lr = 0.001, \n",
    "                 critic_lr = 0.0001,\n",
    "                 variance = 1.0):\n",
    "        super(SAC,self).__init__()\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.batch = batch\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.noise_flag = 1.0\n",
    "        self.std = np.sqrt(variance)\n",
    "        \n",
    "        \n",
    "        #spaces\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.combined_space = self.action_space + self.state_space\n",
    "        \n",
    "        # replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.state_space,self.action_space,capacity,self.batch)\n",
    "        \n",
    "        # optimizers\n",
    "        self.opt_actor = tf.keras.optimizers.Adam(actor_lr)\n",
    "        self.opt_critic = tf.keras.optimizers.Adam(critic_lr)\n",
    "        \n",
    "        #losses\n",
    "        self.loss_actor = self.loss_actor_func\n",
    "        self.loss_critic = tf.keras.losses.MSE\n",
    "        \n",
    "        # models\n",
    "        self.critic = Critic()        \n",
    "        self.actor = Actor(self.action_space)\n",
    "        #self.critic.compile(optimizer = self.opt_critic,loss = self.loss_critic)\n",
    "        #self.actor.compile(optimizer = self.opt_actor,loss = self.loss_actor)\n",
    "        \n",
    "        \n",
    "        #print('model: ',self.critic.summary())\n",
    "        # target models\n",
    "        self.target_actor = Actor(self.action_space)\n",
    "        self.target_critic = Critic()         \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        #cpc\n",
    "        #self.cpc = CPC(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "    \n",
    "    def loss_actor_func(self,states,actions):\n",
    "        actions = self.actor(states)\n",
    "        #stateactions = tf.concat([states,actions],-1)\n",
    "        #print(\"state,action shape: \",states.shape,actions.shape)\n",
    "        Q = self.critic([states,actions])\n",
    "        loss = - tf.reduce_mean(Q)\n",
    "        return loss\n",
    "        \n",
    "    def update_actor(self,states,actions):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_actor(states,actions)\n",
    "\n",
    "        grad = tape.gradient(loss,self.actor.trainable_variables)\n",
    "        self.opt_actor.apply_gradients(zip(grad, self.actor.trainable_variables))\n",
    "        #print('actor loss: ', loss ,\"\\n\" )\n",
    "        return loss\n",
    "    \n",
    "    def set_noise_flag(self,num):\n",
    "        self.noise_flag = np.float32(not not num)\n",
    "    \n",
    "    def continous_noise(self):\n",
    "        #num = np.random.normal(0,self.std)\n",
    "        #result = np.full((self.action_space,),num)\n",
    "        result = np.random.normal(0,self.std,size=(self.action_space,))\n",
    "        return self.noise_flag * np.clip(result,a_min = -1.0, a_max = 1.0)\n",
    "    \n",
    "    def update_critic(self,states_i,actions_i,Q_h):\n",
    "        match = Q_h.shape[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q = self.critic([states_i,actions_i])\n",
    "            Q = tf.reshape(Q,(1,1,1,match))\n",
    "            Q_h = tf.reshape(Q_h,(1,1,1,match))\n",
    "            loss = self.loss_critic(Q,Q_h)\n",
    "\n",
    "        grad = tape.gradient(loss,self.critic.trainable_variables)\n",
    "        #grad_magnitude = tf.reduce_sum(grad)\n",
    "        self.opt_critic.apply_gradients(zip(grad, self.critic.trainable_variables))\n",
    "        #print('critic loss: ', loss ,\"\\n\" )\n",
    "        #print(\"check exploding gradient: \", grad)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def store_replay(self,state,next_state,action,reward,done):\n",
    "        self.replay_buffer.add(state,next_state,action,reward,done)\n",
    "    \n",
    "    def set_labels(self,states_i,next_states_i,actions_i,rewards_i,terminal_i):\n",
    "        mu = self.target_actor(next_states_i)\n",
    "        #print('ends: ', terminal)\n",
    "        #print(mu,states)\n",
    "#         stateactions = tf.concat([states,mu],1)\n",
    "        Q_h = self.target_critic([next_states_i,mu])\n",
    "        y = rewards_i + terminal_i*self.gamma * Q_h\n",
    "        #y = np.concatenate(self.y,0).astype('float32') #.reshape((self.minibatch_size,1,1,1))\n",
    "        #print('y: ',self.y)\n",
    "        #y = tf.reshape(y,(self.replay_buffer.batch,1,1,1))\n",
    "        return y \n",
    "\n",
    "    \n",
    "    def update_target_weights(self):   \n",
    "        tgt_critic_weight = self.target_critic.get_weights()\n",
    "        tgt_actor_weight = self.target_actor.get_weights()\n",
    "        actor_weight = self.actor.get_weights()\n",
    "        critic_weight = self.critic.get_weights()\n",
    "        \n",
    "        \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_actor_weight,actor_weight)):\n",
    "            tgt_actor_weight[idx] = self.tau*part_tgt + (1.0-self.tau)*part_net\n",
    "            \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_critic_weight,critic_weight)):\n",
    "            tgt_critic_weight[idx] = self.tau*part_tgt + (1.0-self.tau)*part_net\n",
    "        \n",
    "\n",
    "            \n",
    "        self.target_actor.set_weights(tgt_actor_weight)\n",
    "        self.target_critic.set_weights(tgt_critic_weight)\n",
    "            \n",
    "    def save(self,filename):\n",
    "        self.actor.save_weights(filename)\n",
    "        self.critic.save_weights(filename)\n",
    "        self.target_actor.save_weights(filename)\n",
    "        self.target_critic.save_weights(filename)\n",
    "    \n",
    "    def load(self,filename):\n",
    "        self.actor.load_weights(filename)\n",
    "        self.critic.load_weights(filename)\n",
    "        self.target_actor.load_weights(filename)\n",
    "        self.target_critic.load_weights(filename)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StY2xL3E73ea"
   },
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, batch_size, terms, predict_terms=1, image_size=64, color=False, rescale=True, aug=True, is_training=True, method='cpc'):\n",
    "        self.batch_size = batch_size\n",
    "        self.terms = terms\n",
    "        self.predict_terms = predict_terms\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "        self.aug = aug\n",
    "        self.is_training = is_training\n",
    "        self.method = method\n",
    "        self.lena = cv2.imread(os.path.join(base_dir,'lena.jpg'))\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        if self.is_training:\n",
    "            self.x = x_train/255.0\n",
    "            self.y = y_train\n",
    "        else:\n",
    "            self.x = x_test/255.0\n",
    "            self.y = y_test\n",
    "        self.idxs = []\n",
    "        for i in range(10):\n",
    "            y = y_train if self.is_training else y_test\n",
    "            self.idxs.append(np.where(y == i)[0])\n",
    "        self.n_samples = len(self.y)//terms if self.method == 'cpc' else len(self.y)\n",
    "        self.shape = self.x.shape\n",
    "        self.n_batches = self.n_samples//batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.cpc_batch() if self.method == 'cpc' else self.benchmark_batch()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def cpc_batch(self):\n",
    "        img_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
    "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
    "        for bi in range(self.batch_size):\n",
    "            seed = np.random.randint(10)\n",
    "            sentence = np.arange(seed, seed + self.terms + self.predict_terms) % 10\n",
    "            if bi < self.batch_size//2:\n",
    "                num = np.arange(10)\n",
    "                predicted = sentence[-self.predict_terms:]\n",
    "                for i, p in enumerate(predicted):\n",
    "                    predicted[i] = np.random.choice(num[num != p], 1)\n",
    "                sentence[-self.predict_terms:] = predicted % 10\n",
    "                sentence_labels[bi, :] = 0\n",
    "            img_labels[bi, :] = sentence\n",
    "        images = self.get_samples(img_labels).reshape((self.batch_size, self.terms+self.predict_terms, self.image_size, self.image_size, 3))\n",
    "        x_images = images[:, :-self.predict_terms, ...]\n",
    "        y_images = images[:, -self.predict_terms:, ...]\n",
    "        idx = np.random.choice(self.batch_size, self.batch_size, replace=False)\n",
    "        return [x_images[idx], y_images[idx]], sentence_labels[idx]\n",
    "\n",
    "    def get_samples(self, img_labels):\n",
    "        idx = []\n",
    "        for label in img_labels.flatten():\n",
    "            idx.append(np.random.choice(self.idxs[int(label)], 1)[0])\n",
    "        img_batch = self.x[idx, :, :]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        return img_batch\n",
    "\n",
    "    def _aug_batch(self, img_batch):\n",
    "        if self.image_size != 28:\n",
    "            resized = []\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                resized.append(cv2.resize(img_batch[i], (self.image_size, self.image_size)))\n",
    "            img_batch = np.stack(resized)\n",
    "        img_batch = img_batch.reshape((img_batch.shape[0], 1, self.image_size, self.image_size))\n",
    "        img_batch = np.concatenate([img_batch, img_batch, img_batch], axis=1)\n",
    "\n",
    "        if self.color:\n",
    "            img_batch[img_batch >= 0.5] = 1\n",
    "            img_batch[img_batch < 0.5] = 0\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                x_c = np.random.randint(0, self.lena.shape[0] - self.image_size)\n",
    "                y_c = np.random.randint(0, self.lena.shape[1] - self.image_size)\n",
    "                img = self.lena[x_c:x_c+self.image_size, y_c:y_c+self.image_size]\n",
    "                img = np.array(img).transpose((2, 0, 1))/255.0\n",
    "                for j in range(3):\n",
    "                    img[j, :, :] = (img[j, :, :] + np.random.uniform(0, 1))/2.0\n",
    "                img[img_batch[i, :, :, :] == 1] = 1 - img[img_batch[i, :, :, :] == 1]\n",
    "                img_batch[i, :, :, :] = img\n",
    "\n",
    "        if self.rescale:\n",
    "            img_batch = img_batch * 2 - 1\n",
    "        img_batch = img_batch.transpose((0, 2, 3, 1))\n",
    "        return img_batch\n",
    "\n",
    "    def benchmark_batch(self):\n",
    "        idx = np.random.choice(len(self.x), self.batch_size, replace=False)\n",
    "        img_batch = self.x[idx]\n",
    "        label_batch = self.y[idx]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        label_batch = label_batch.reshape((-1, 1))\n",
    "        return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TbyX3qlk1b4g",
    "outputId": "5ee8316c-0a96-4277-b003-e9937032d726"
   },
   "outputs": [],
   "source": [
    "# #train loop\n",
    "# dh_train = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=True, method='cpc')\n",
    "# dh_test = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=False, method='cpc')\n",
    "# accuracy_metric_train = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_train = tf.keras.metrics.BinaryCrossentropy()\n",
    "# accuracy_metric_test = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_test = tf.keras.metrics.BinaryCrossentropy()\n",
    "# cpc = CPCModel(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "# optim = tf.keras.optimizers.Adam(1e-3)\n",
    "# cb = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
    "#       tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
    "#                                           monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
    "#       tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
    "#       tf.keras.callbacks.TensorBoard()]\n",
    "# cpc.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "# cpc.fit(x=dh_train, epochs=10, validation_data=dh_test, steps_per_epoch=60000//64, validation_steps=10000//64, callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vn5Hapce1cE5"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtara\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# train loop params\n",
    "\n",
    "\n",
    "episodes = 200\n",
    "episode_steps = 1000\n",
    "buffer_size = 100000\n",
    "batch_size = 16\n",
    "\n",
    "# pybullet setup\n",
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "env.seed(seed)\n",
    "#env.render() #mode = 'human')\n",
    "env._max_episode_steps = episode_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 6\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "writer_reward = tf.summary.create_file_writer(reward_dir)\n",
    "\n",
    "#get spaces\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "print(state_space,action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0eguufNR5p4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  0  :episode:  0\n",
      "q_loss:  [[[0.50974935]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (406,300) (400,300) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ff4a6cd83208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m#update target nets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0msac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ac750ab378f1>\u001b[0m in \u001b[0;36mupdate_target_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart_tgt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpart_net\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_critic_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcritic_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             \u001b[0mtgt_critic_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpart_tgt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpart_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (406,300) (400,300) "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "sac = SAC(action_space=action_space,\n",
    "          state_space=state_space,\n",
    "          capacity = buffer_size,\n",
    "          batch = batch_size,\n",
    "          tau = 0.999,\n",
    "          gamma = 0.99,\n",
    "          actor_lr = 0.0001,\n",
    "          critic_lr = 0.001,\n",
    "          variance = 0.2)\n",
    "\n",
    "#fill replay buffer\n",
    "#env._max_episode_steps = buffer_size\n",
    "#sac.replay_buffer.fill_buffer(buffer_size, state, episode_steps) # self,timesteps,state,prev_timesteps\n",
    "#env._max_episode_steps = episode_steps\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(env, \"baseline_training\", video_callable=lambda episode: True, force=\"true\")\n",
    "state = env.reset()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    sumreward = 0\n",
    "    for step in range(episode_steps):\n",
    "        #print(observation)\n",
    "        print('t: ',step, ' :episode: ',episode)\n",
    "        #print('state: ',state)\n",
    "        \n",
    "        # get action\n",
    "        state = tf.reshape(state,(1,1,state_space)) #,dtype='float32')\n",
    "        #print(state)\n",
    "        tensor_noisy_action = sac.actor(state)+sac.continous_noise()\n",
    "        #tensor_action = tf.clip_by_value(tensor_action, clip_value_min=-1.0, clip_value_max=1.0)\n",
    "\n",
    "        noisy_action = tensor_noisy_action[0][0]\n",
    "        #print('action: ',action)\n",
    "        \n",
    "        #get loss\n",
    "        #q_loss = sac.critic(state,tensor_action)\n",
    "        \n",
    "        \n",
    "        # execute action\n",
    "        next_state, reward, done, info = env.step(noisy_action)\n",
    "        sumreward += reward\n",
    "\n",
    "        # store transitions\n",
    "        sac.store_replay(state,next_state,noisy_action,reward,done)\n",
    "        \n",
    "        #print('state: ',state)\n",
    "        #print('next_state: ',next_state)\n",
    "        #print('action: ',action)\n",
    "        #print('reward: ',reward)\n",
    "\n",
    "        #sample minibatch from data\n",
    "        states_i,next_states_i,actions_i,rewards_i,terminal_i = sac.replay_buffer.sample()\n",
    "        \n",
    "        #set labels y_i\n",
    "        y = sac.set_labels(states_i,next_states_i,actions_i,rewards_i,terminal_i)\n",
    "        #print('y: ',y)\n",
    "        \n",
    "        # update critic net\n",
    "        q_loss = sac.update_critic(states_i, actions_i, y)\n",
    "\n",
    "        print('q_loss: ', q_loss.numpy())\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Squared QLosses (qtarget - qval)^2', q_loss[0][0][0].numpy(),\n",
    "                              step=episode * episode_steps + step + 1)\n",
    "        \n",
    "        #losses[episode*timesteps + t] = loss\n",
    "        #losses[i_episode*timesteps+] = history.history\n",
    "        \n",
    "        #update actor net\n",
    "        sac.update_actor(states_i, sac.actor(states_i)) #actions)\n",
    "        #print('weight check: ',rl.actor.get_weights(),'\\n')\n",
    "        \n",
    "        #update target nets\n",
    "        sac.update_target_weights()\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #rewards[episode] = sumreward\n",
    "            #sac.save(base_dir+'/baseline_model')\n",
    "            print(\"Episode {} finished after {} timesteps with average reward {}\".format(episode,step+1,sumreward))\n",
    "            with writer_reward.as_default():\n",
    "                tf.summary.scalar('Episode sum reward', sumreward,step=episode)\n",
    "            break\n",
    "print('done') \n",
    "sac.save(base_dir+'/baseline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aFHYhHwRxH6"
   },
   "source": [
    " https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss(InfoNCE Loss )\n",
    "<br>\n",
    "Representation Learning with Contrastive Predictive Coding\n",
    "<br>\n",
    "https://github.com/gdao-research/cpc/blob/master/cpc/data_handler.py (CPC)\n",
    "<br>\n",
    "https://github.com/davidtellez/contrastive-predictive-coding/blob/master/train_model.py (CPC)\n",
    "<br>\n",
    "https://github.com/MishaLaskin/curl/blob/23b0880708c29b078b0a25e62ff31fb587587b18/utils.py#L123 (replay buffer and SAC)\n",
    "<br>\n",
    "https://github.com/marload/DeepRL-TensorFlow2/blob/master/A2C/A2C_Discrete.py (A2C)\n",
    "<br>\n",
    "https://github.com/germain-hug/Deep-RL-Keras/blob/master/A3C/a3c.py (A3C)\n",
    "<br>\n",
    "https://github.com/tensorflow/agents/blob/v0.5.0/tf_agents/agents/sac/sac_agent.py (SAC)\n",
    "<br>\n",
    "https://github.com/cookbenjamin/DDPG/blob/master/networks/critic.py (transfer the action state merge to second layer)\n",
    "<br>\n",
    "https://github.com/georgesung/TD3 (check expected results)\n",
    "<br>\n",
    "https://github.com/georgesung/TD3/blob/master/DDPG.py (param mistake)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CPCprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:csci-7000-rl] *",
   "language": "python",
   "name": "conda-env-csci-7000-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
