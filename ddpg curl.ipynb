{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "z57IXf9q1Ovh",
    "outputId": "1b2cbade-88b2-48d7-b64c-3ab7d29c1715"
   },
   "outputs": [],
   "source": [
    "# if colab\n",
    "\n",
    "# !pip install pybullet\n",
    "# !pip install gym\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t8ezUpd1SIy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np \n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "import pybullet\n",
    "import pybullet_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from skimage.util.shape import view_as_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "y0_9dbk8Cbnv",
    "outputId": "4cdb116a-5cec-4456-dc79-f07bb48d9fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "seed = 654765645\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# check if GPU\n",
    "print(tf.config.list_physical_devices())\n",
    "#tf.config.set_memory_growth(physical_devices[1], True)\n",
    "\n",
    "#set parameters\n",
    "episodes = 200\n",
    "episode_steps = 1000\n",
    "buffer_size = 10000\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hG1Ruv5FCgDv",
    "outputId": "4fe5a6b2-b4e6-4241-e5ca-c412cf4637a7"
   },
   "outputs": [],
   "source": [
    "# colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = \"drive/My Drive/\"\n",
    "# base_dir = root_dir + 'CPCtesting'\n",
    "# os.makedirs(base_dir,exist_ok=True)\n",
    "\n",
    "# train_dir = base_dir + '/train'\n",
    "# os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "# model_dir = base_dir + '/model'\n",
    "# os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "# if local machine\n",
    "base_dir = \".\"\n",
    "\n",
    "train_dir = base_dir + '/train'\n",
    "os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "model_dir = base_dir + '/model'\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "logs_base_dir = base_dir + '/logs'\n",
    "\n",
    "log_dir = base_dir + '/training_logs_save'\n",
    "\n",
    "reward_dir = base_dir + '/training_rewards_save'\n",
    "\n",
    "\n",
    "# colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = \"drive/My Drive/\"\n",
    "# base_dir = root_dir + 'CPCtesting'\n",
    "# os.makedirs(base_dir,exist_ok=True)\n",
    "\n",
    "# train_dir = base_dir + '/train'\n",
    "# os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "# model_dir = base_dir + '/model'\n",
    "# os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "# if local machine\n",
    "base_dir = \".\"\n",
    "\n",
    "train_dir = base_dir + '/train'\n",
    "os.makedirs(train_dir,exist_ok=True)\n",
    "\n",
    "model_dir = base_dir + '/model'\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "\n",
    "logs_base_dir = base_dir + '/logs'\n",
    "\n",
    "log_dir = base_dir + '/training_logs_save'\n",
    "\n",
    "reward_dir = base_dir + '/training_rewards_save'\n",
    "\n",
    "cpc_dir = base_dir + '/training_cpcloss_save'\n",
    "\n",
    "\n",
    "# remove old logs\n",
    "fileList1 = glob.glob(os.path.join(log_dir , \"events.*\"))\n",
    "fileList2 = glob.glob(os.path.join(reward_dir , \"events.*\"))\n",
    "fileList3 = glob.glob(os.path.join(cpc_dir,\"events.*\"))\n",
    "\n",
    "for filePath in fileList1:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except:\n",
    "        print(\"Error while deleting file : \", filePath)\n",
    "        \n",
    "for filePath in fileList2:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except:\n",
    "        print(\"Error while deleting file : \", filePath)\n",
    "for filePath in fileList3:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except:\n",
    "        print(\"Error while deleting file : \", filePath)\n",
    "\n",
    "# tensorboard directories\n",
    "# %load_ext tensorboard\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "os.makedirs(reward_dir,exist_ok=True)\n",
    "os.makedirs(cpc_dir,exist_ok=True)\n",
    "# %tensorboard --logdir {logs_base_dir}\n",
    "\n",
    "# tensorboard\n",
    "#%load_ext tensorboard\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "#%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWEWANi66t_w"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIKY-VyC1bsu"
   },
   "outputs": [],
   "source": [
    "class CPCModel(tf.keras.Model):\n",
    "    def __init__(self,code_size, predict_terms, terms=4, units=256, image_size=64, channels=3):\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.code_size = code_size\n",
    "        self.predict_terms = predict_terms\n",
    "        self.terms = terms\n",
    "        self.units = units\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu1 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu2 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu3 = tf.keras.layers.LeakyReLU()\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='linear')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu4 = tf.keras.layers.LeakyReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense5 = tf.keras.layers.Dense(units=256, activation='linear')\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.lrelu5 = tf.keras.layers.LeakyReLU()\n",
    "        self.dense6 = tf.keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=False, name='ar_context')\n",
    "        self.linear = tf.keras.layers.Dense(predict_terms*code_size, activation='linear')    \n",
    "   \n",
    "    def encoding(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.lrelu5(x)\n",
    "        z = self.dense6(x)\n",
    "        #print(\"encode_shape: \",z.shape)\n",
    "        return z\n",
    "  \n",
    "    def get_context(self, x):\n",
    "        z = self.encoding(x)\n",
    "        print(\"shape check: \", z.shape, \":terms: \", self.terms, \": code size: \", self.code_size)\n",
    "        z = tf.reshape(z, [-1, self.terms, self.code_size])\n",
    "        c = self.gru(z)\n",
    "        return c\n",
    "    def get_prediction(self, x):\n",
    "        c = self.get_context(x)\n",
    "        z_hats = self.linear(c)\n",
    "        z_hat = tf.reshape(z_hats, [-1, self.predict_terms, self.code_size])\n",
    "        return z_hat\n",
    "\n",
    "    def loss(self,weights,biases,labels,inputs,num_samples,num_classes): \n",
    "        loss = tf.nn.nce_loss(\n",
    "        weights, biases, labels, inputs, num_sampled, num_classes, num_true=1,\n",
    "        sampled_values=None, remove_accidental_hits=False, name='nce_loss')\n",
    "        return loss\n",
    "  \n",
    "    def loss_cpc(self,inputs):\n",
    "        prob = self.call(inputs)\n",
    "        #prob = logits - np.max(logits, axis=1)\n",
    "        print('prob shape: ', prob.shape)\n",
    "        labels = np.arange(prob.shape[0])\n",
    "        print('logit shape test: ',prob.shape,\": \",labels.shape)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(labels,prob) #, axis=-1, name=None)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def call_logits(self,inputs):\n",
    "        x_tm, x_tp = inputs # get two images\n",
    "        x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels]) # reshape image\n",
    "        x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels]) # reshape image\n",
    "        z_hat = self.get_prediction(x_tm)\n",
    "        z_tp = self.encoding(x_tp)\n",
    "        z_tp = tf.reshape(z_tp, [-1, self.predict_terms, self.code_size])\n",
    "        logits = tf.reduce_mean(tf.reduce_mean(z_hat*z_tp, axis=-1), axis=-1, keepdims=True)\n",
    "        #probs = tf.sigmoid(dot_prods)\n",
    "        return logits\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x_tm, x_tp = inputs\n",
    "        #x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels])\n",
    "        #x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels])\n",
    "        z_hat = self.get_prediction(x_tm)\n",
    "        z_tp = self.encoding(x_tp)\n",
    "        z_tp = tf.reshape(z_tp, [-1, self.predict_terms, self.code_size])\n",
    "        dot_prods = tf.reduce_mean(tf.reduce_mean(z_hat*z_tp, axis=-1), axis=-1, keepdims=True)\n",
    "        probs = tf.sigmoid(dot_prods)\n",
    "        return probs\n",
    "\n",
    "\n",
    "  # def save(self):\n",
    "  #       f1 = os.path.join(folder,'target_actor')\n",
    "  #       f2 = os.path.join(folder, 'target_critic')\n",
    "  #       f3 = os.path.join(folder, 'actor')\n",
    "  #       f4 = os.path.join(folder, 'critic')\n",
    "  #       self.target_actor.save(f1)\n",
    "  #       self.target_critic.save(f2)\n",
    "  #       self.actor.save(f3)\n",
    "  #       self.critic.save(f4)\n",
    "\n",
    "\n",
    "  # def load(self):\n",
    "  #   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wW6bj9SJkd20"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,state_space,action_space,capacity,batch,image_size,channels):\n",
    "        self.capacity = capacity\n",
    "        self.batch = batch\n",
    "        self.elements = 0\n",
    "        \n",
    "        self.avaliable_batch = 0\n",
    "        self.idx = 0\n",
    "        self.entries = 0 \n",
    "        \n",
    "        self.states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.next_states = np.empty((self.capacity,state_space),dtype = np.float32)\n",
    "        self.actions = np.empty((self.capacity,action_space),dtype = np.float32)\n",
    "        self.rewards = np.empty((self.capacity,1),dtype = np.float32)\n",
    "        self.not_dones = np.empty((self.capacity, 1), dtype=np.float32)\n",
    "        self.image_states = np.empty((self.capacity, image_size,image_size,channels), dtype=np.float32)\n",
    "        self.image_next_states = np.empty((self.capacity, image_size,image_size,channels), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def add(self,state,next_state,action,reward,done,state_image,next_state_image):\n",
    "        np.copyto(self.states[self.idx], state)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_states[self.idx], next_state)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "        np.copyto(self.image_states[self.idx],state_image)\n",
    "        np.copyto(self.image_next_states[self.idx],state_image)\n",
    "        #self.avaliable_batch= (self.avaliable_batch + 1) if self.avaliable_batch < self.batch else self.batch\n",
    "        #self.entries = (self.entries + 1) if self.entries < self.capacity else self.capacity\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.entries = np.minimum(self.entries + 1, self.capacity)\n",
    "        \n",
    "    def sample(self):\n",
    "        num = self.entries\n",
    "        if(num > self.batch):\n",
    "            num = self.batch\n",
    "        #print('avaliable_batch: ',self.avaliable_batch, \"entries: \", self.entries,'capacity: ', self.capacity)\n",
    "        idx = np.random.choice(self.entries,size = num,replace=False)\n",
    "        #print('test idx: ', idx)\n",
    "        \n",
    "        states = tf.convert_to_tensor(self.states[idx])\n",
    "        next_states = tf.convert_to_tensor(self.next_states[idx])\n",
    "        actions = tf.convert_to_tensor(self.actions[idx])\n",
    "        rewards = tf.convert_to_tensor(self.rewards[idx])\n",
    "        not_dones = tf.convert_to_tensor(self.not_dones[idx])\n",
    "        image_states = tf.convert_to_tensor(self.image_states[idx])\n",
    "        image_next_states = tf.convert_to_tensor(self.image_next_states[idx])\n",
    "        \n",
    "        return states,next_states,actions,rewards,not_dones,image_states,image_next_states                    \n",
    "        \n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,action_space):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        #params\n",
    "        self.action_space = action_space\n",
    "       \n",
    "        #model\n",
    "        self.dense1 = tf.keras.layers.Dense(400,\n",
    "                                            #input_shape = (None,1,1,state_space),\n",
    "                                            activation = 'relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed)\n",
    "                                           )\n",
    "        self.dense2 = tf.keras.layers.Dense(300,\n",
    "                                            activation='relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed)\n",
    "                                            )\n",
    "        self.dense3 = tf.keras.layers.Dense(self.action_space,\n",
    "                                            bias_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                             maxval=0.003,\n",
    "                                                                                             seed = seed\n",
    "                                                                                            ),\n",
    "                                            kernel_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                               maxval=0.003,\n",
    "                                                                                               seed = seed\n",
    "                                                                                              )\n",
    "                                           )\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=-1)\n",
    "        \n",
    "    def call(self,x):\n",
    "        [state,encode_state] = x\n",
    "        x = self.concat1([state,encode_state])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic,self).__init__()\n",
    "        # layers\n",
    "        self.dense1 = tf.keras.layers.Dense(400,\n",
    "                                            #input_shape=(1,1,combined_space),\n",
    "                                            activation = 'relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                           )\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.dense2 = tf.keras.layers.Dense(300,\n",
    "                                            activation='relu',\n",
    "                                            #bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            bias_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                     mode='fan_in', \n",
    "                                                                                                     distribution='uniform', \n",
    "                                                                                                     seed=seed\n",
    "                                                                                                    ),\n",
    "                                            kernel_initializer = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                                                                       mode='fan_in', \n",
    "                                                                                                       distribution='uniform', \n",
    "                                                                                                       seed=seed\n",
    "                                                                                                      ),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                            )\n",
    "        self.dense3 = tf.keras.layers.Dense(1,\n",
    "                                            bias_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003),\n",
    "                                            kernel_initializer = tf.random_uniform_initializer(minval=-0.003, \n",
    "                                                                                               maxval=0.003,\n",
    "                                                                                               seed = seed\n",
    "                                                                                              ),\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "                                            ) \n",
    "    #predict\n",
    "    def call(self,data): #states,actions):\n",
    "        [states,actions] = data\n",
    "        #x = tf.concat([states,actions],-1)\n",
    "        y = self.dense1(states)\n",
    "        x = self.concat1([y,actions])\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DDPG():\n",
    "    def __init__(self,\n",
    "                 state_space,\n",
    "                 action_space,\n",
    "                 image_size,\n",
    "                 cropped_image_size,\n",
    "                 capacity = 1000,\n",
    "                 batch = 1, \n",
    "                 tau=0.999,\n",
    "                 gamma=0.99,\n",
    "                 actor_lr = 0.001, \n",
    "                 critic_lr = 0.0001,\n",
    "                 variance = 1.0,\n",
    "                 code_size=128, \n",
    "                 predict_terms=4,\n",
    "                 terms=4, \n",
    "                 units=256,\n",
    "                 channels=3):\n",
    "        super(DDPG,self).__init__()\n",
    "        \n",
    "        #ddpg hyperparameters\n",
    "        self.batch = batch\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.noise_flag = 1.0\n",
    "        self.std = np.sqrt(variance)\n",
    "        \n",
    "        #cpc hyperparameters\n",
    "        self.image_size = image_size\n",
    "        self.cropped_image_size = cropped_image_size\n",
    "        self.code_size = code_size\n",
    "        self.predict_terms = predict_terms\n",
    "        self.terms = terms\n",
    "        self.units = units\n",
    "        self.channels = channels\n",
    "        \n",
    "        #spaces\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.combined_space = self.action_space + self.state_space\n",
    "        \n",
    "        # replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.state_space,\n",
    "                                          self.action_space,\n",
    "                                          capacity,\n",
    "                                          self.batch,\n",
    "                                          self.image_size,\n",
    "                                          self.channels)\n",
    "        \n",
    "        # optimizers\n",
    "        self.opt_actor = tf.keras.optimizers.Adam(actor_lr)\n",
    "        self.opt_critic = tf.keras.optimizers.Adam(critic_lr)\n",
    "        self.opt_cpc = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "        \n",
    "        #losses\n",
    "        self.loss_actor = self.loss_actor_func\n",
    "        self.loss_critic = tf.keras.losses.MSE\n",
    "        #self.loss_cpc = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "        \n",
    "        #cpc model\n",
    "        self.cpc = CPCModel(code_size=self.code_size, \n",
    "                            predict_terms=self.predict_terms, \n",
    "                            terms=self.terms, \n",
    "                            units=self.units, \n",
    "                            image_size=self.cropped_image_size, \n",
    "                            channels=self.channels)\n",
    "        \n",
    "        self.cb_cpc = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
    "              tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
    "                                                  monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
    "              tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
    "              tf.keras.callbacks.TensorBoard()]\n",
    "        #self.cpc.compile(optimizer=self.opt_cpc, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #rl models\n",
    "        self.critic = Critic()        \n",
    "        self.actor = Actor(self.action_space)\n",
    "        #self.critic.compile(optimizer = self.opt_critic,loss = self.loss_critic)\n",
    "        #self.actor.compile(optimizer = self.opt_actor,loss = self.loss_actor)\n",
    "        \n",
    "        \n",
    "        #print('model: ',self.critic.summary())\n",
    "        # target models\n",
    "        self.target_actor = Actor(self.action_space)\n",
    "        self.target_critic = Critic()         \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        #cpc\n",
    "        #self.cpc = CPC(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "    \n",
    "    def loss_actor_func(self,states_tuple,actions):\n",
    "        [states_i,encode_states_i] = states_tuple\n",
    "        actions = self.actor(states_tuple)\n",
    "        #stateactions = tf.concat([states,actions],-1)\n",
    "        #print(\"state,action shape: \",states.shape,actions.shape)\n",
    "        Q = self.critic([states_i,actions])\n",
    "        loss = - tf.reduce_mean(Q)\n",
    "        return loss\n",
    "        \n",
    "    def update_actor(self,states,actions):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_actor(states,actions)\n",
    "\n",
    "        grad = tape.gradient(loss,self.actor.trainable_variables)\n",
    "        self.opt_actor.apply_gradients(zip(grad, self.actor.trainable_variables))\n",
    "        #print('actor loss: ', loss ,\"\\n\" )\n",
    "        return loss\n",
    "    \n",
    "    def set_noise_flag(self,num):\n",
    "        self.noise_flag = np.float32(not not num)\n",
    "    \n",
    "    def continous_noise(self):\n",
    "        #num = np.random.normal(0,self.std)\n",
    "        #result = np.full((self.action_space,),num)\n",
    "        result = np.random.normal(0,self.std,size=(self.action_space,))\n",
    "        return self.noise_flag * np.clip(result,a_min = -1.0, a_max = 1.0)\n",
    "    \n",
    "    def update_critic(self,states_i,actions_i,Q_h):\n",
    "        match = Q_h.shape[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q = self.critic([states_i,actions_i])\n",
    "            Q = tf.reshape(Q,(1,1,1,match))\n",
    "            Q_h = tf.reshape(Q_h,(1,1,1,match))\n",
    "            loss = self.loss_critic(Q,Q_h)\n",
    "\n",
    "        grad = tape.gradient(loss,self.critic.trainable_variables)\n",
    "        #grad_magnitude = tf.reduce_sum(grad)\n",
    "        self.opt_critic.apply_gradients(zip(grad, self.critic.trainable_variables))\n",
    "        #print('critic loss: ', loss ,\"\\n\" )\n",
    "        #print(\"check exploding gradient: \", grad)\n",
    "        return loss\n",
    "    \n",
    "    def random_crop(self,images,output_size): # thanks to CURL developers for the random crop\n",
    "            \"\"\"\n",
    "            Vectorized way to do random crop using sliding windows\n",
    "            and picking out random ones\n",
    "            args:\n",
    "                imgs, batch images with shape (B,C,H,W)\n",
    "            \"\"\"\n",
    "            \"\"\" Image shape (batch,width,height,channels)\"\"\"\n",
    "            # batch size\n",
    "            n = images.shape[0]\n",
    "            img_size = images.shape[1]\n",
    "            crop_max = img_size - output_size\n",
    "            print('crop_max: ', crop_max)\n",
    "            images = images.numpy() #np.transpose(images, (0, 2, 3, 1))\n",
    "            w1 = np.random.randint(0, crop_max, n)\n",
    "            h1 = np.random.randint(0, crop_max, n)\n",
    "            # creates all sliding windows combinations of size (output_size)\n",
    "            windows = view_as_windows(\n",
    "                images, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n",
    "            # selects a random window for each batch element\n",
    "            cropped_imgs = windows[np.arange(n), w1, h1]\n",
    "            cropped_imgs = np.reshape(cropped_imgs,(-1,self.cropped_image_size,self.cropped_image_size,self.channels))\n",
    "            print(\"cropped image shape: \",cropped_imgs.shape)\n",
    "            return cropped_imgs\n",
    "    \n",
    "#     def compute_logits(self, z_a, z_pos):\n",
    "#         \"\"\"\n",
    "#         Uses logits trick for CURL:\n",
    "#         - compute (B,B) matrix z_a (W z_pos.T)\n",
    "#         - positives are all diagonal elements\n",
    "#         - negatives are all other elements\n",
    "#         - to compute loss use multiclass cross entropy with identity matrix for labels\n",
    "#         \"\"\"\n",
    "#         #Wz = tf.linalg.matmul(W, z_pos.T)  # (z_dim,B)\n",
    "#         logits = tf.linalg.matmul(z_a, tf.transpose(z_pos))  # (B,B)\n",
    "#         print(\"logits shape: \", logits.shape)\n",
    "#         logits = logits - tf.math.maximum(logits, 1)[0][:, None]\n",
    "#         return logits\n",
    "    \n",
    "    \n",
    "    def update_cpc(self,image_states):\n",
    "        state_anchor = self.random_crop(image_states,self.cropped_image_size)\n",
    "        state_pos = self.random_crop(image_states,self.cropped_image_size)\n",
    "        \n",
    "        #print(\"state image shape: \",state_anchor.shape ,\": \",state_pos.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            #prob = self.cpc([state_anchor,state_pos])\n",
    "            #z_a = self.cpc.encoding(state_anchor)\n",
    "            #z_pos = self.cpc.encoding(state_pos)\n",
    "\n",
    "            #logits = tf.cast(self.compute_logits(z_a,z_pos),dtype=\"float32\")\n",
    "            #labels = np.arange(prob.shape[0])\n",
    "            #labels = tf.cast(tf.reshape(labels,(labels.shape[0],1)),dtype=\"float32\")\n",
    "            #print('logit shape:', prob.shape,\":labels: \",labels.shape)\n",
    "            cross_entropy = self.cpc.loss_cpc([state_anchor,state_pos]) #labels,prob)\n",
    "            loss = -tf.reduce_mean(cross_entropy)\n",
    "        print(\"loss test: \", loss)\n",
    "        grad = tape.gradient(cross_entropy,self.cpc.trainable_variables)\n",
    "        #print(\"grad: \", grad)\n",
    "        self.opt_cpc.apply_gradients(zip(grad,self.cpc.trainable_variables))\n",
    "        return loss\n",
    "        \n",
    "        #logits = self.cpc.loss(z_a, z_pos)\n",
    "        #cpc.fit(data,x=dh_train, epochs=10, validation_data=dh_test, steps_per_epoch=60000//64, validation_steps=10000//64, callbacks=cb)\n",
    "\n",
    "    \n",
    "    def store_replay(self,state,next_state,action,reward,done,state_image,next_state_image):\n",
    "        self.replay_buffer.add(state,next_state,action,reward,done,state_image,next_state_image)\n",
    "    \n",
    "    def set_labels(self,states_i,next_states_i,actions_i,rewards_i,terminal_i,state_image_i,next_state_image_i):\n",
    "        cropped_state_image_i = ddpg.random_crop(state_image_i,self.cropped_image_size)\n",
    "        encode_state_i = self.cpc.encoding(cropped_state_image_i)\n",
    "        mu = self.target_actor([next_states_i,encode_state_i])\n",
    "        #print('ends: ', terminal)\n",
    "        #print(mu,states)\n",
    "#         stateactions = tf.concat([states,mu],1)\n",
    "        Q_h = self.target_critic([next_states_i,mu])\n",
    "        y = rewards_i + terminal_i*self.gamma * Q_h\n",
    "        #y = np.concatenate(self.y,0).astype('float32') #.reshape((self.minibatch_size,1,1,1))\n",
    "        #print('y: ',self.y)\n",
    "        #y = tf.reshape(y,(self.replay_buffer.batch,1,1,1))\n",
    "        return y \n",
    "\n",
    "    \n",
    "    def update_target_weights(self):   \n",
    "        tgt_critic_weight = self.target_critic.get_weights()\n",
    "        tgt_actor_weight = self.target_actor.get_weights()\n",
    "        actor_weight = self.actor.get_weights()\n",
    "        critic_weight = self.critic.get_weights()\n",
    "        \n",
    "        \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_actor_weight,actor_weight)):\n",
    "            tgt_actor_weight[idx] = self.tau*part_tgt + (1.0-self.tau)*part_net\n",
    "            \n",
    "        for idx,(part_tgt,part_net) in enumerate(zip(tgt_critic_weight,critic_weight)):\n",
    "            tgt_critic_weight[idx] = self.tau*part_tgt + (1.0-self.tau)*part_net\n",
    "        \n",
    "\n",
    "            \n",
    "        self.target_actor.set_weights(tgt_actor_weight)\n",
    "        self.target_critic.set_weights(tgt_critic_weight)\n",
    "            \n",
    "    def save(self,filename):\n",
    "        self.actor.save_weights(filename)\n",
    "        self.critic.save_weights(filename)\n",
    "        self.target_actor.save_weights(filename)\n",
    "        self.target_critic.save_weights(filename)\n",
    "    \n",
    "    def load(self,filename):\n",
    "        self.actor.load_weights(filename)\n",
    "        self.critic.load_weights(filename)\n",
    "        self.target_actor.load_weights(filename)\n",
    "        self.target_critic.load_weights(filename)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StY2xL3E73ea"
   },
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, \n",
    "                 batch_size, \n",
    "                 terms, \n",
    "                 predict_terms=1, \n",
    "                 image_size=64, \n",
    "                 color=False, \n",
    "                 rescale=True, \n",
    "                 aug=True, \n",
    "                 is_training=True, \n",
    "                 method='cpc'):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.terms = terms\n",
    "        self.predict_terms = predict_terms\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "        self.aug = aug\n",
    "        self.is_training = is_training\n",
    "        self.method = method\n",
    "        self.lena = cv2.imread(os.path.join(base_dir,'lena.jpg'))\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        if self.is_training:\n",
    "            self.x = x_train/255.0\n",
    "            self.y = y_train\n",
    "        else:\n",
    "            self.x = x_test/255.0\n",
    "            self.y = y_test\n",
    "        self.idxs = []\n",
    "        for i in range(10):\n",
    "            y = y_train if self.is_training else y_test\n",
    "            self.idxs.append(np.where(y == i)[0])\n",
    "        self.n_samples = len(self.y)//terms if self.method == 'cpc' else len(self.y)\n",
    "        self.shape = self.x.shape\n",
    "        self.n_batches = self.n_samples//batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.cpc_batch() if self.method == 'cpc' else self.benchmark_batch()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def cpc_batch(self):\n",
    "        img_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
    "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
    "        for bi in range(self.batch_size):\n",
    "            seed = np.random.randint(10)\n",
    "            sentence = np.arange(seed, seed + self.terms + self.predict_terms) % 10\n",
    "            if bi < self.batch_size//2:\n",
    "                num = np.arange(10)\n",
    "                predicted = sentence[-self.predict_terms:]\n",
    "                for i, p in enumerate(predicted):\n",
    "                    predicted[i] = np.random.choice(num[num != p], 1)\n",
    "                sentence[-self.predict_terms:] = predicted % 10\n",
    "                sentence_labels[bi, :] = 0\n",
    "            img_labels[bi, :] = sentence\n",
    "        images = self.get_samples(img_labels).reshape((self.batch_size, \n",
    "                                                       self.terms+self.predict_terms, \n",
    "                                                       self.image_size, \n",
    "                                                       self.image_size, \n",
    "                                                       3))\n",
    "        x_images = images[:, :-self.predict_terms, ...]\n",
    "        y_images = images[:, -self.predict_terms:, ...]\n",
    "        idx = np.random.choice(self.batch_size, self.batch_size, replace=False)\n",
    "        return [x_images[idx], y_images[idx]], sentence_labels[idx]\n",
    "\n",
    "    def get_samples(self, img_labels):\n",
    "        idx = []\n",
    "        for label in img_labels.flatten():\n",
    "            idx.append(np.random.choice(self.idxs[int(label)], 1)[0])\n",
    "        img_batch = self.x[idx, :, :]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        return img_batch\n",
    "\n",
    "    def _aug_batch(self, img_batch):\n",
    "        if self.image_size != 28:\n",
    "            resized = []\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                resized.append(cv2.resize(img_batch[i], (self.image_size, self.image_size)))\n",
    "            img_batch = np.stack(resized)\n",
    "        img_batch = img_batch.reshape((img_batch.shape[0], 1, self.image_size, self.image_size))\n",
    "        img_batch = np.concatenate([img_batch, img_batch, img_batch], axis=1)\n",
    "\n",
    "        if self.color:\n",
    "            img_batch[img_batch >= 0.5] = 1\n",
    "            img_batch[img_batch < 0.5] = 0\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                x_c = np.random.randint(0, self.lena.shape[0] - self.image_size)\n",
    "                y_c = np.random.randint(0, self.lena.shape[1] - self.image_size)\n",
    "                img = self.lena[x_c:x_c+self.image_size, y_c:y_c+self.image_size]\n",
    "                img = np.array(img).transpose((2, 0, 1))/255.0\n",
    "                for j in range(3):\n",
    "                    img[j, :, :] = (img[j, :, :] + np.random.uniform(0, 1))/2.0\n",
    "                img[img_batch[i, :, :, :] == 1] = 1 - img[img_batch[i, :, :, :] == 1]\n",
    "                img_batch[i, :, :, :] = img\n",
    "\n",
    "        if self.rescale:\n",
    "            img_batch = img_batch * 2 - 1\n",
    "        img_batch = img_batch.transpose((0, 2, 3, 1))\n",
    "        return img_batch\n",
    "\n",
    "    def benchmark_batch(self):\n",
    "        idx = np.random.choice(len(self.x), self.batch_size, replace=False)\n",
    "        img_batch = self.x[idx]\n",
    "        label_batch = self.y[idx]\n",
    "        if self.aug:\n",
    "            img_batch = self._aug_batch(img_batch)\n",
    "        label_batch = label_batch.reshape((-1, 1))\n",
    "        return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TbyX3qlk1b4g",
    "outputId": "5ee8316c-0a96-4277-b003-e9937032d726"
   },
   "outputs": [],
   "source": [
    "# #train loop\n",
    "# dh_train = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=True, method='cpc')\n",
    "# dh_test = DataHandler(64, 4, predict_terms=4, image_size=64, color=True, rescale=True, aug=True, is_training=False, method='cpc')\n",
    "# accuracy_metric_train = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_train = tf.keras.metrics.BinaryCrossentropy()\n",
    "# accuracy_metric_test = tf.keras.metrics.BinaryAccuracy()\n",
    "# loss_metric_test = tf.keras.metrics.BinaryCrossentropy()\n",
    "# cpc = CPCModel(code_size=128, predict_terms=4, terms=4, units=256, image_size=64, channels=3)\n",
    "# optim = tf.keras.optimizers.Adam(1e-3)\n",
    "# cb = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4),\n",
    "#       tf.keras.callbacks.ModelCheckpoint('weights/weights.{epoch:02d}-{val_binary_accuracy:.2f}.cpkt',\n",
    "#                                           monitor='val_binary_accuracy', save_best_only=True, save_weights_only=True),\n",
    "#       tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3),\n",
    "#       tf.keras.callbacks.TensorBoard()]\n",
    "# cpc.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "# cpc.fit(x=dh_train, epochs=10, validation_data=dh_test, steps_per_epoch=60000//64, validation_steps=10000//64, callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vn5Hapce1cE5"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtara\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# train loop params\n",
    "\n",
    "episodes = 100\n",
    "episode_steps = 1000\n",
    "\n",
    "# buffer params\n",
    "buffer_size = 10000\n",
    "\n",
    "# network params\n",
    "batch_size = 8\n",
    "tau = 0.999\n",
    "\n",
    "\n",
    "# image parameter\n",
    "image_size = 100\n",
    "cropped_image_size = 84\n",
    "\n",
    "# pybullet setup\n",
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "#env.render(mode = 'human')\n",
    "env._max_episode_steps = episode_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 6\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "writer_reward = tf.summary.create_file_writer(reward_dir)\n",
    "writer_cpc = tf.summary.create_file_writer(cpc_dir)\n",
    "\n",
    "#get spaces\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "print(state_space,action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0eguufNR5p4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  0  :episode:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop_max:  16\n",
      "cropped image shape:  (1, 84, 84, 4)\n",
      "test image:  (1, 84, 84, 4)\n",
      "crop_max:  16\n",
      "cropped image shape:  (1, 84, 84, 4)\n",
      "q_loss:  [0.6398181]\n",
      "crop_max:  16\n",
      "cropped image shape:  (1, 84, 84, 4)\n",
      "test shape state_image_i (1, 84, 84, 4)\n",
      "crop_max:  16\n",
      "cropped image shape:  (1, 84, 84, 4)\n",
      "crop_max:  16\n",
      "cropped image shape:  (1, 84, 84, 4)\n",
      "shape check:  (1, 128) :terms:  4 : code size:  128\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 128 values, but the requested shape requires a multiple of 512 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e605fb262b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m#print('weight check: ',rl.actor.get_weights(),'\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mcpc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_cpc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_image_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test shape cpc loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpc_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mwriter_cpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e96d42826c11>\u001b[0m in \u001b[0;36mupdate_cpc\u001b[1;34m(self, image_states)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;31m#labels = tf.cast(tf.reshape(labels,(labels.shape[0],1)),dtype=\"float32\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m#print('logit shape:', prob.shape,\":labels: \",labels.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_cpc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_anchor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_pos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#labels,prob)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss test: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7ebe19a97dd9>\u001b[0m in \u001b[0;36mloss_cpc\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss_cpc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;31m#prob = logits - np.max(logits, axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prob shape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7ebe19a97dd9>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m#x_tm = tf.reshape(x_tm, [-1, self.image_size, self.image_size, self.channels])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m#x_tp = tf.reshape(x_tp, [-1, self.image_size, self.image_size, self.channels])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mz_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mz_tp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mz_tp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_tp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7ebe19a97dd9>\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mz_hats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mz_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7ebe19a97dd9>\u001b[0m in \u001b[0;36mget_context\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape check: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":terms: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\": code size: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m   \"\"\"\n\u001b[1;32m--> 193\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7434\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7435\u001b[0m         return reshape_eager_fallback(\n\u001b[1;32m-> 7436\u001b[1;33m             tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   7437\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7438\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   7461\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tshape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7462\u001b[0m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 7463\u001b[1;33m                              ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m   7464\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7465\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\csci-7000-rl\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 128 values, but the requested shape requires a multiple of 512 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "ddpg = DDPG(action_space=action_space,\n",
    "          state_space=state_space,\n",
    "          capacity = buffer_size,\n",
    "          batch = batch_size,\n",
    "          tau = 0.999,\n",
    "          gamma = 0.99,\n",
    "          actor_lr = 0.0001,\n",
    "          critic_lr = 0.001,\n",
    "          variance = 0.3,\n",
    "          image_size = image_size,\n",
    "          cropped_image_size = cropped_image_size,\n",
    "          code_size=128, \n",
    "          predict_terms=4,\n",
    "          terms=4, \n",
    "          units=256,\n",
    "          channels=4)\n",
    "\n",
    "#fill replay buffer\n",
    "#env._max_episode_steps = buffer_size\n",
    "#ddpg.replay_buffer.fill_buffer(buffer_size, state, episode_steps) # self,timesteps,state,prev_timesteps\n",
    "#env._max_episode_steps = episode_steps\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(env, \"baseline_training\", video_callable=lambda episode: True, force=\"true\")\n",
    "state = env.reset()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    sumreward = 0\n",
    "    for step in range(episode_steps):\n",
    "        #print(observation)\n",
    "        print('t: ',step, ' :episode: ',episode)\n",
    "        #print('state: ',state)\n",
    "        \n",
    "        # get action\n",
    "        state = tf.cast(tf.reshape(state,(1,1,state_space)),dtype='float32')\n",
    "        state_image = pybullet.getCameraImage(image_size ,image_size)[2] # get rgb image\n",
    "        plt.imshow(state_image)\n",
    "        plt.show()\n",
    "        state_image = tf.reshape(state_image, (1,ddpg.image_size,ddpg.image_size,ddpg.channels))\n",
    "        state_image = tf.cast(state_image, dtype= \"float32\") / 255.0\n",
    "        \n",
    "        cropped_state_image = ddpg.random_crop(state_image,ddpg.cropped_image_size)\n",
    "        \n",
    "        \n",
    "        print(\"test image: \",cropped_state_image.shape)\n",
    "        encode_state = ddpg.cpc.encoding(cropped_state_image)\n",
    "        encode_state = tf.reshape(encode_state,(1,1,ddpg.code_size))\n",
    "        #print(state)\n",
    "        tensor_action = ddpg.actor([state,encode_state]) + ddpg.continous_noise() #ddpg.actor(state)+ddpg.continous_noise()\n",
    "        action = tensor_action[0][0]\n",
    "        #print('action: ',action)\n",
    "        \n",
    "        #get loss\n",
    "        #q_loss = ddpg.critic(state,tensor_action)\n",
    "        \n",
    "        # get image state\n",
    "        #state_image = pybullet.getCameraImage(image_size ,image_size)[2] # get rgb image\n",
    "        #print('state_image: ',state_image,':',type(state_image))\n",
    "        \n",
    "        # execute action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # get image next state\n",
    "        next_state_image = pybullet.getCameraImage(image_size ,image_size)[2] # get rgb image\n",
    "        next_state_image = tf.reshape(next_state_image, (-1,ddpg.image_size,ddpg.image_size,ddpg.channels))\n",
    "        next_state_image = tf.cast(next_state_image, dtype= \"float32\") / 255.0\n",
    "\n",
    "        # sum reward\n",
    "        sumreward += reward\n",
    "\n",
    "        # store transitions\n",
    "        ddpg.store_replay(state,next_state,action,reward,done,state_image,next_state_image)\n",
    "        \n",
    "        #print('state: ',state)\n",
    "        #print('next_state: ',next_state)\n",
    "        #print('action: ',action)\n",
    "        #print('reward: ',reward)\n",
    "\n",
    "        #sample minibatch from data\n",
    "        states_i,next_states_i,actions_i,rewards_i,terminal_i,state_image_i,next_state_image_i = ddpg.replay_buffer.sample()\n",
    "        \n",
    "        #set labels y_i\n",
    "        y = ddpg.set_labels(states_i,next_states_i,actions_i,rewards_i,terminal_i,state_image_i,next_state_image_i)\n",
    "        \n",
    "        # update critic net\n",
    "        q_loss = ddpg.update_critic(states_i, actions_i, y)\n",
    "\n",
    "        print('q_loss: ', q_loss[0][0].numpy())\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Squared QLosses (qtarget - qval)^2', q_loss[0][0][0].numpy(),\n",
    "                              step=episode * episode_steps + step + 1)\n",
    "        \n",
    "        #get cpc encoder\n",
    "        cropped_state_image_i = ddpg.random_crop(state_image_i,ddpg.cropped_image_size)\n",
    "        print('test shape state_image_i', cropped_state_image_i.shape)\n",
    "        encode_state = ddpg.cpc.encoding(cropped_state_image_i)\n",
    "        \n",
    "        #update actor net\n",
    "        ddpg.update_actor([states_i,encode_state],ddpg.actor([states_i,encode_state]))\n",
    "        #print('weight check: ',rl.actor.get_weights(),'\\n')\n",
    "        \n",
    "        cpc_loss = ddpg.update_cpc(state_image_i)\n",
    "        print('test shape cpc loss: ', cpc_loss)\n",
    "        with writer_cpc.as_default():\n",
    "            tf.summary.scalar('CPC Loss', cpc_loss,\n",
    "                              step=episode * episode_steps + step + 1)\n",
    "        \n",
    "        #update target nets\n",
    "        ddpg.update_target_weights()\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #rewards[episode] = sumreward\n",
    "            #ddpg.save(base_dir+'/baseline_model')\n",
    "            print(\"Episode {} finished after {} timesteps with reward {}\".format(episode,step+1,sumreward))\n",
    "            with writer_reward.as_default():\n",
    "                tf.summary.scalar('Episode sum reward', sumreward,step=episode)\n",
    "            break\n",
    "print('done') \n",
    "ddpg.save(base_dir + '/baseline_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aFHYhHwRxH6"
   },
   "source": [
    " https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss(InfoNCE Loss )\n",
    "<br>\n",
    "Representation Learning with Contrastive Predictive Coding\n",
    "<br>\n",
    "https://github.com/gdao-research/cpc/blob/master/cpc/data_handler.py (CPC)\n",
    "<br>\n",
    "https://github.com/davidtellez/contrastive-predictive-coding/blob/master/train_model.py (CPC)\n",
    "<br>\n",
    "https://github.com/MishaLaskin/curl/blob/23b0880708c29b078b0a25e62ff31fb587587b18/utils.py#L123 (replay buffer and SAC)\n",
    "<br>\n",
    "https://github.com/marload/DeepRL-TensorFlow2/blob/master/A2C/A2C_Discrete.py (A2C)\n",
    "<br>\n",
    "https://github.com/germain-hug/Deep-RL-Keras/blob/master/A3C/a3c.py (A3C)\n",
    "<br>\n",
    "https://github.com/tensorflow/agents/blob/v0.5.0/tf_agents/agents/sac/sac_agent.py (SAC)\n",
    "<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate (needed to combine inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CPCprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:csci-7000-rl] *",
   "language": "python",
   "name": "conda-env-csci-7000-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
